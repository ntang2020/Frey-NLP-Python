{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6090f31-a58d-4b62-a9ec-6489c35a7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####download packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc1c733-c2ff-49d3-a13a-a6ae8e5ac6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2f577cc-fd91-4e60-91fe-acce320a698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###classifer function below: this class is the main classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9faad24d-d84d-413a-a31c-608a864314a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_NLC_Classifer():\n",
    "    def __init__(self, k=1, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(1) nearest neighbour algorithm and\n",
    "    # returns the label with closest match. \n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            max_sim = 0\n",
    "            max_index = 0\n",
    "            for j in range(self.x_train.shape[0]):\n",
    "                temp = self.document_similarity(x_test[i], self.x_train[j])\n",
    "                if temp > max_sim:\n",
    "                    max_sim = temp\n",
    "                    max_index = j\n",
    "            y_predict.append(self.y_train[max_index])\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb197176-05c2-4b98-a3f5-8183e9fb7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###bag of words: these two functions below essentially tag and create the corpus. \n",
    "###A synsets as a variation of a corpus for this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf46f26f-8303-4cf1-8323-7acc29793fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(self, doc):\n",
    "        \"\"\"\n",
    "            Returns a list of synsets in document.\n",
    "            Tokenizes and tags the words in the document doc.\n",
    "            Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "        \"\"\"\n",
    "        tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "        l = []\n",
    "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "        for token, tag in zip(tokens, tags):\n",
    "            syntag = self.convert_tag(tag[1])\n",
    "            syns = wn.synsets(token, syntag)\n",
    "            if (len(syns) > 0):\n",
    "                l.append(syns[0])\n",
    "        return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c20bc6-1375-4e15-80a1-3d8b1a3b1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "###This function calculates the similarity score of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9806b357-e165-41d3-b81e-d34891902a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "    \"\"\"\n",
    "          Calculate the normalized similarity score of s1 onto s2\n",
    "          For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "          Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "          number of largest similarity values found.\n",
    "\n",
    "          Args:\n",
    "              s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "          Returns:\n",
    "              normalized similarity score of s1 onto s2\n",
    "          \"\"\"\n",
    "    s1_largest_scores = []\n",
    "\n",
    "    for i, s1_synset in enumerate(s1, 0):\n",
    "        max_score = 0\n",
    "        for s2_synset in s2:\n",
    "            if distance_type == 'path':\n",
    "                score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "            else:\n",
    "                score = s1_synset.wup_similarity(s2_synset)                  \n",
    "            if score != None:\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "              \n",
    "        if max_score != 0:\n",
    "            s1_largest_scores.append(max_score)\n",
    "          \n",
    "    mean_score = np.mean(s1_largest_scores)\n",
    "                 \n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae23ae9-245c-4fb9-a28e-8fe7527af595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculates the similarity score between the documents as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b444a3e-ddd4-4d77-b190-c164df8cda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_similarity(self,doc1, doc2):\n",
    "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "    synsets1 = self.doc_to_synsets(doc1)\n",
    "    synsets2 = self.doc_to_synsets(doc2)\n",
    "\n",
    "    return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2200641f-af29-4d88-85ae-583660f021cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            text       answer  output\n",
      "0           How hot is it today?  temperature       1\n",
      "1             Is it hot outside?  temperature       1\n",
      "2  Will it be uncomfortably hot?  temperature       1\n",
      "3         Will it be sweltering?  temperature       1\n",
      "4          How cold is it today?  temperature       1\n",
      "\n",
      "Size of input file is  (50, 3)\n"
     ]
    }
   ],
   "source": [
    "# 1. Importing the dataset\n",
    "#we'll use the demo dataset available at Watson NLC Classifier Demo.\n",
    "FILENAME = \"https://raw.githubusercontent.com/watson-developer-cloud/natural-language-classifier-nodejs/master/training/weather_data_train.csv\"          \n",
    "\n",
    "dataset = pd.read_csv(FILENAME, header = None)\n",
    "\n",
    "dataset.rename(columns = {0:'text', 1:'answer'}, inplace = True)\n",
    "\n",
    "dataset['output'] = np.where(dataset['answer'] == 'temperature', 1,0)\n",
    "Num_Words = dataset.shape[0]\n",
    "\n",
    "print(dataset.head())\n",
    "print(\"\\nSize of input file is \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8d01bf8-1150-4d64-96d5-80b1612fa5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### the transcript cleaning section: this will remove uneccesary characters and words in the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e2f70-0882-4c4f-857b-6d3dc121bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "956e22af-ff9d-4700-b75c-ae3704400178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the sample of training text after removing the stop words\n",
      "0                     hot\n",
      "1                     hot\n",
      "2       uncomfortably hot\n",
      "3              sweltering\n",
      "4                    cold\n",
      "5                    cold\n",
      "6      uncomfortably cold\n",
      "7                  frigid\n",
      "8           expected high\n",
      "9    expected temperature\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('stopwords')\n",
    "s = stopwords.words('english')\n",
    "#add additional stop words\n",
    "s.extend(['today', 'tomorrow', 'outside', 'out', 'there'])\n",
    "ps = nltk.wordnet.WordNetLemmatizer()\n",
    "for i in range(dataset.shape[0]):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset.loc[i,'text'])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    dataset.loc[i, 'text'] = review\n",
    "X_train = dataset['text']\n",
    "y_train = dataset['output']\n",
    "print(\"Below is the sample of training text after removing the stop words\")\n",
    "print(dataset['text'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed463cd5-9617-4b5b-b538-07364a528f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####This section trains the model and predicts the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cc17e36-4685-45ba-af83-3b914a890c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNN_NLC_Classifer' object has no attribute 'document_similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_81180/3885735019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtest_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0my_pred_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred_final\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_81180/2054750477.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_sim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mmax_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KNN_NLC_Classifer' object has no attribute 'document_similarity'"
     ]
    }
   ],
   "source": [
    "# 4. Train the Classifier\n",
    "classifier = KNN_NLC_Classifer(k=1, distance_type='path')\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "final_test_list = ['will it rain', 'Is it hot outside?' , 'What is the expected high for today?' , \n",
    "                   'Will it be foggy tomorrow?', 'Should I prepare for sleet?',\n",
    "                     'Will there be a storm today?', 'do we need to take umbrella today',\n",
    "                    'will it be wet tomorrow', 'is it humid tomorrow', 'what is the precipitation today',\n",
    "                    'is it freezing outside', 'is it cool outside', \"are there strong winds outside\",]\n",
    "                 \n",
    "test_corpus = []\n",
    "for i in range(len(final_test_list)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', final_test_list[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
    "    review = ' '.join(review)\n",
    "    test_corpus.append(review)\n",
    "\n",
    "y_pred_final = classifier.predict(test_corpus)\n",
    "\n",
    "output_df = pd.DataFrame(data = {'text': final_test_list, 'code': y_pred_final})\n",
    "output_df['answer'] = np.where(output_df['code']==1, 'Temperature','Conditions')\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287f30c-bfd6-431e-9500-94a88850b924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55f8ea-49fd-4b5f-b74a-c6f719a72f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
