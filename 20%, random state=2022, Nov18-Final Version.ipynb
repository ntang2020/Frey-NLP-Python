{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data (replies already been excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets (excludes replies) before data pre-processing: 6501\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('Jan6(excludes replies).csv')\n",
    "text2 = pd.read_csv('Frey transcripts.csv')\n",
    "print('The number of tweets (excludes replies) before data pre-processing:',len(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "      <th>Company</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>1/5/21 15:59</td>\n",
       "      <td>transcript 1</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134648...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>scene_2.wav\\nElizabeth: [00:00:01] This is my ...</td>\n",
       "      <td>12/30/20 23:14</td>\n",
       "      <td>transcript 2</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134442...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>scene_3.wav\\nLindsey: [00:00:03] Before I was ...</td>\n",
       "      <td>12/23/20 19:00</td>\n",
       "      <td>transcript 3</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>12/16/20 21:26</td>\n",
       "      <td>transcript 4</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133932...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...</td>\n",
       "      <td>12/15/20 20:16</td>\n",
       "      <td>transcript 5</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133894...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6501 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             tweets  \\\n",
       "0            0.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "1            1.0  scene_2.wav\\nElizabeth: [00:00:01] This is my ...   \n",
       "2            2.0  scene_3.wav\\nLindsey: [00:00:03] Before I was ...   \n",
       "3            4.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "4            5.0  Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...   \n",
       "...          ...                                                ...   \n",
       "6496         NaN                                                NaN   \n",
       "6497         NaN                                                NaN   \n",
       "6498         NaN                                                NaN   \n",
       "6499         NaN                                                NaN   \n",
       "6500         NaN                                                NaN   \n",
       "\n",
       "                date       Company  \\\n",
       "0       1/5/21 15:59  transcript 1   \n",
       "1     12/30/20 23:14  transcript 2   \n",
       "2     12/23/20 19:00  transcript 3   \n",
       "3     12/16/20 21:26  transcript 4   \n",
       "4     12/15/20 20:16  transcript 5   \n",
       "...              ...           ...   \n",
       "6496             NaN           NaN   \n",
       "6497             NaN           NaN   \n",
       "6498             NaN           NaN   \n",
       "6499             NaN           NaN   \n",
       "6500             NaN           NaN   \n",
       "\n",
       "                                                    url  \n",
       "0     https://twitter.com/MemphisMeats/status/134648...  \n",
       "1     https://twitter.com/MemphisMeats/status/134442...  \n",
       "2     https://twitter.com/MemphisMeats/status/134182...  \n",
       "3     https://twitter.com/MemphisMeats/status/133932...  \n",
       "4     https://twitter.com/MemphisMeats/status/133894...  \n",
       "...                                                 ...  \n",
       "6496                                                NaN  \n",
       "6497                                                NaN  \n",
       "6498                                                NaN  \n",
       "6499                                                NaN  \n",
       "6500                                                NaN  \n",
       "\n",
       "[6501 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda import guidedlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "1. tokenization\n",
    "2. remove @users, hashtag symbols, Urls, and special symbols(i.e., '&amp'), non-alphabetic characters, and words that have less than 3 characters\n",
    "3. remove stopwords\n",
    "4. lowercase transformation\n",
    "5. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tweets after cleaning: 6501\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "porter = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "b = []\n",
    "for i,u in text2.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split(): #tokenization\n",
    "        if '@' not in words: #remove @users\n",
    "            words = words.replace('#','') #remove hashtag symbol\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words)#remove non-alphabetic characters\n",
    "                        if len(words)>2:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if len(token) >2: # remove words that have less than 3 characters\n",
    "            token = token.lower()# lowercase form\n",
    "            if token not in stop_words:# remove stopwords\n",
    "                token = porter.stem(token) #stemming\n",
    "                doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text2['processed']=[i for i in b]\n",
    "\n",
    "# exclude tweets that are not in English\n",
    "non_english_list = ['temiz','rkiy','erik','nda','konu','dan','da','ba','al','viand','para','na','dann','uft','laboratorio','dieser','kalbimi',\n",
    "                   'restoranda','evento','komo','ind','tica','futuro','sonra','yla','cre','ili','daki',\n",
    "                   'zaman']\n",
    "index_axis = []\n",
    "for index,i in text2.iterrows():\n",
    "    if len(i['processed']) == 0:\n",
    "        index_axis.append(index)\n",
    "    else:\n",
    "        for word in i['processed'].split():\n",
    "            if word in non_english_list:\n",
    "                index_axis.append(index)\n",
    "                break\n",
    "text2.drop(text2.index[index_axis],inplace=True)\n",
    "print(\"number of tweets after cleaning:\",len(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\n",
       "transcript 1     1\n",
       "transcript 10    1\n",
       "transcript 11    1\n",
       "transcript 12    1\n",
       "transcript 13    1\n",
       "transcript 14    1\n",
       "transcript 2     1\n",
       "transcript 3     1\n",
       "transcript 4     1\n",
       "transcript 5     1\n",
       "transcript 6     1\n",
       "transcript 7     1\n",
       "transcript 8     1\n",
       "transcript 9     1\n",
       "Name: tweets, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2.groupby('Company')['tweets'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly Select 20% Dataset As Our Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_random_20percent = text2.sample(frac=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company\n",
       "transcript 2    1\n",
       "transcript 3    1\n",
       "Name: tweets, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_random_20percent.groupby('Company')['tweets'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.phrases:collecting all words and their counts\n",
      "INFO:gensim.models.phrases:PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO:gensim.models.phrases:collected 829 token types (unigram + bigrams) from a corpus of 1856 words and 1300 sentences\n",
      "INFO:gensim.models.phrases:merged Phrases<829 vocab, min_count=1, threshold=1, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:Phrases lifecycle event {'msg': 'built Phrases<829 vocab, min_count=1, threshold=1, max_vocab_size=40000000> in 0.00s', 'datetime': '2022-07-14T13:23:35.345434', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.models.phrases:exporting phrases from Phrases<829 vocab, min_count=1, threshold=1, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<32 phrases, min_count=1, threshold=1> from Phrases<829 vocab, min_count=1, threshold=1, max_vocab_size=40000000> in 0.00s', 'datetime': '2022-07-14T13:23:35.348414', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n",
      "INFO:gensim.corpora.dictionary:built Dictionary<315 unique tokens: ['nan', 'activ', 'adhes', 'adopt', 'agenc']...> from 1300 documents (total 1809 corpus positions)\n",
      "INFO:gensim.utils:Dictionary lifecycle event {'msg': \"built Dictionary<315 unique tokens: ['nan', 'activ', 'adhes', 'adopt', 'agenc']...> from 1300 documents (total 1809 corpus positions)\", 'datetime': '2022-07-14T13:23:35.360036', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.1\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.1\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 10 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-9.703 per-word bound, 833.6 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.100): 0.014*\"realli\" + 0.012*\"elizabeth\" + 0.011*\"move\" + 0.011*\"know\" + 0.009*\"two\" + 0.009*\"get\" + 0.009*\"see\" + 0.008*\"feel\" + 0.008*\"week\" + 0.008*\"lindsey\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.100): 0.503*\"nan\" + 0.007*\"elizabeth\" + 0.005*\"know\" + 0.004*\"day\" + 0.004*\"two\" + 0.004*\"week\" + 0.003*\"breath\" + 0.003*\"like\" + 0.003*\"call\" + 0.003*\"wait\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.100): 0.018*\"nan\" + 0.013*\"elizabeth\" + 0.009*\"breath\" + 0.009*\"two\" + 0.009*\"know\" + 0.008*\"oncologist\" + 0.008*\"much\" + 0.008*\"agenc\" + 0.007*\"feel\" + 0.007*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.100): 0.055*\"nan\" + 0.014*\"realli\" + 0.011*\"lindsey\" + 0.011*\"like\" + 0.011*\"know\" + 0.011*\"move\" + 0.010*\"call\" + 0.010*\"see\" + 0.008*\"one\" + 0.008*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.100): 0.015*\"know\" + 0.014*\"realli\" + 0.011*\"move\" + 0.011*\"like\" + 0.009*\"call\" + 0.009*\"lindsey\" + 0.009*\"elizabeth\" + 0.009*\"get\" + 0.008*\"week\" + 0.008*\"feel\"\n",
      "INFO:gensim.models.ldamodel:topic diff=3.668762, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=10, decay=0.5, chunksize=2000> in 0.23s', 'datetime': '2022-07-14T13:23:35.768648', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.09090909090909091\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.09090909090909091\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 11 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-10.112 per-word bound, 1106.9 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.091): 0.613*\"nan\" + 0.006*\"elizabeth\" + 0.004*\"know\" + 0.003*\"two\" + 0.003*\"day\" + 0.003*\"breath\" + 0.003*\"week\" + 0.002*\"wait\" + 0.002*\"seat\" + 0.002*\"feel\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.091): 0.014*\"elizabeth\" + 0.014*\"realli\" + 0.012*\"know\" + 0.012*\"move\" + 0.011*\"two\" + 0.010*\"feel\" + 0.009*\"week\" + 0.009*\"get\" + 0.008*\"see\" + 0.007*\"everyth\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.091): 0.063*\"nan\" + 0.016*\"know\" + 0.014*\"realli\" + 0.013*\"lindsey\" + 0.012*\"like\" + 0.011*\"maria\" + 0.010*\"see\" + 0.009*\"surgeri\" + 0.009*\"get\" + 0.009*\"move\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.091): 0.020*\"nan\" + 0.013*\"elizabeth\" + 0.009*\"breath\" + 0.009*\"two\" + 0.009*\"know\" + 0.008*\"oncologist\" + 0.008*\"much\" + 0.007*\"agenc\" + 0.007*\"feel\" + 0.007*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.091): 0.698*\"nan\" + 0.003*\"know\" + 0.002*\"like\" + 0.002*\"realli\" + 0.002*\"elizabeth\" + 0.002*\"see\" + 0.002*\"move\" + 0.002*\"lindsey\" + 0.002*\"call\" + 0.002*\"maria\"\n",
      "INFO:gensim.models.ldamodel:topic diff=4.055264, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=11, decay=0.5, chunksize=2000> in 0.23s', 'datetime': '2022-07-14T13:23:36.009356', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.08333333333333333\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.08333333333333333\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 12 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-10.518 per-word bound, 1466.2 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.083): 0.033*\"nan\" + 0.015*\"realli\" + 0.012*\"know\" + 0.011*\"move\" + 0.011*\"like\" + 0.010*\"lindsey\" + 0.010*\"call\" + 0.010*\"see\" + 0.009*\"elizabeth\" + 0.008*\"one\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.083): 0.720*\"nan\" + 0.003*\"know\" + 0.002*\"see\" + 0.002*\"realli\" + 0.002*\"like\" + 0.002*\"lindsey\" + 0.002*\"move\" + 0.002*\"maria\" + 0.002*\"breast\" + 0.002*\"call\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.083): 0.115*\"nan\" + 0.014*\"lindsey\" + 0.014*\"realli\" + 0.013*\"know\" + 0.012*\"maria\" + 0.011*\"like\" + 0.011*\"see\" + 0.010*\"surgeri\" + 0.009*\"could\" + 0.009*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.083): 0.600*\"nan\" + 0.007*\"elizabeth\" + 0.004*\"know\" + 0.003*\"two\" + 0.003*\"day\" + 0.003*\"breath\" + 0.003*\"week\" + 0.003*\"wait\" + 0.003*\"feel\" + 0.003*\"nurs\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.083): 0.015*\"know\" + 0.014*\"realli\" + 0.012*\"move\" + 0.011*\"like\" + 0.010*\"lindsey\" + 0.009*\"call\" + 0.009*\"get\" + 0.009*\"week\" + 0.008*\"elizabeth\" + 0.008*\"feel\"\n",
      "INFO:gensim.models.ldamodel:topic diff=4.736691, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=12, decay=0.5, chunksize=2000> in 0.23s', 'datetime': '2022-07-14T13:23:36.249605', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.07692307692307693\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.07692307692307693\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 13 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-10.941 per-word bound, 1965.6 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.077): 0.017*\"elizabeth\" + 0.013*\"realli\" + 0.012*\"know\" + 0.011*\"two\" + 0.010*\"feel\" + 0.010*\"move\" + 0.009*\"week\" + 0.008*\"oncologist\" + 0.008*\"get\" + 0.008*\"breath\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.077): 0.024*\"nan\" + 0.018*\"elizabeth\" + 0.008*\"know\" + 0.008*\"breath\" + 0.007*\"two\" + 0.007*\"feel\" + 0.006*\"day\" + 0.006*\"nurs\" + 0.006*\"oncologist\" + 0.006*\"wait\"\n",
      "INFO:gensim.models.ldamodel:topic #11 (0.077): 0.015*\"know\" + 0.012*\"lindsey\" + 0.011*\"maria\" + 0.011*\"move\" + 0.011*\"see\" + 0.011*\"like\" + 0.010*\"realli\" + 0.009*\"breast\" + 0.009*\"gone\" + 0.008*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.077): 0.015*\"elizabeth\" + 0.013*\"know\" + 0.011*\"realli\" + 0.008*\"like\" + 0.008*\"week\" + 0.007*\"call\" + 0.007*\"breath\" + 0.007*\"feel\" + 0.007*\"see\" + 0.007*\"move\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.077): 0.761*\"nan\" + 0.002*\"elizabeth\" + 0.001*\"know\" + 0.001*\"realli\" + 0.001*\"like\" + 0.001*\"two\" + 0.001*\"breath\" + 0.001*\"day\" + 0.001*\"week\" + 0.001*\"move\"\n",
      "INFO:gensim.models.ldamodel:topic diff=5.284251, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=13, decay=0.5, chunksize=2000> in 0.22s', 'datetime': '2022-07-14T13:23:36.487259', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.07142857142857142\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.07142857142857142\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 14 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-11.364 per-word bound, 2636.1 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.071): 0.034*\"nan\" + 0.015*\"realli\" + 0.012*\"know\" + 0.011*\"like\" + 0.011*\"move\" + 0.010*\"call\" + 0.010*\"lindsey\" + 0.010*\"elizabeth\" + 0.010*\"see\" + 0.009*\"week\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.071): 0.014*\"nan\" + 0.014*\"realli\" + 0.012*\"elizabeth\" + 0.012*\"move\" + 0.011*\"know\" + 0.009*\"two\" + 0.009*\"see\" + 0.009*\"get\" + 0.009*\"week\" + 0.008*\"feel\"\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.071): 0.568*\"nan\" + 0.007*\"know\" + 0.005*\"realli\" + 0.004*\"call\" + 0.004*\"get\" + 0.004*\"move\" + 0.004*\"like\" + 0.003*\"elizabeth\" + 0.003*\"see\" + 0.003*\"two\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.071): 0.021*\"elizabeth\" + 0.012*\"know\" + 0.012*\"feel\" + 0.012*\"two\" + 0.010*\"breath\" + 0.009*\"much\" + 0.009*\"week\" + 0.008*\"agenc\" + 0.008*\"wait\" + 0.008*\"realli\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.071): 0.966*\"nan\" + 0.000*\"elizabeth\" + 0.000*\"know\" + 0.000*\"feel\" + 0.000*\"two\" + 0.000*\"week\" + 0.000*\"realli\" + 0.000*\"nurs\" + 0.000*\"breath\" + 0.000*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic diff=6.215264, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=14, decay=0.5, chunksize=2000> in 0.24s', 'datetime': '2022-07-14T13:23:36.754268', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.06666666666666667\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.06666666666666667\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 15 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-11.799 per-word bound, 3562.5 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.067): 0.635*\"nan\" + 0.006*\"elizabeth\" + 0.003*\"know\" + 0.003*\"two\" + 0.003*\"breath\" + 0.003*\"day\" + 0.002*\"week\" + 0.002*\"wait\" + 0.002*\"feel\" + 0.002*\"seat\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.067): 0.091*\"nan\" + 0.016*\"lindsey\" + 0.015*\"realli\" + 0.014*\"know\" + 0.013*\"maria\" + 0.012*\"like\" + 0.012*\"see\" + 0.011*\"surgeri\" + 0.011*\"could\" + 0.010*\"complet\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.067): 0.973*\"nan\" + 0.000*\"elizabeth\" + 0.000*\"feel\" + 0.000*\"know\" + 0.000*\"two\" + 0.000*\"nurs\" + 0.000*\"breath\" + 0.000*\"kind\" + 0.000*\"day\" + 0.000*\"week\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.067): 0.038*\"nan\" + 0.016*\"realli\" + 0.012*\"move\" + 0.012*\"like\" + 0.012*\"lindsey\" + 0.011*\"know\" + 0.010*\"see\" + 0.010*\"call\" + 0.008*\"week\" + 0.008*\"one\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.067): 0.045*\"nan\" + 0.018*\"elizabeth\" + 0.012*\"realli\" + 0.011*\"two\" + 0.011*\"know\" + 0.011*\"feel\" + 0.009*\"move\" + 0.009*\"week\" + 0.008*\"breath\" + 0.008*\"oncologist\"\n",
      "INFO:gensim.models.ldamodel:topic diff=6.411703, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=15, decay=0.5, chunksize=2000> in 0.22s', 'datetime': '2022-07-14T13:23:36.992117', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.0625\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.0625\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 16 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-12.240 per-word bound, 4836.8 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.062): 0.016*\"elizabeth\" + 0.014*\"know\" + 0.012*\"realli\" + 0.009*\"like\" + 0.009*\"week\" + 0.008*\"call\" + 0.008*\"feel\" + 0.008*\"breath\" + 0.007*\"see\" + 0.007*\"move\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.062): 0.979*\"nan\" + 0.000*\"elizabeth\" + 0.000*\"feel\" + 0.000*\"know\" + 0.000*\"two\" + 0.000*\"nurs\" + 0.000*\"breath\" + 0.000*\"day\" + 0.000*\"kind\" + 0.000*\"week\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.062): 0.767*\"nan\" + 0.003*\"elizabeth\" + 0.002*\"know\" + 0.002*\"day\" + 0.002*\"two\" + 0.002*\"breath\" + 0.001*\"week\" + 0.001*\"wait\" + 0.001*\"seat\" + 0.001*\"feel\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.062): 0.018*\"elizabeth\" + 0.011*\"breath\" + 0.011*\"two\" + 0.010*\"know\" + 0.010*\"oncologist\" + 0.010*\"much\" + 0.009*\"agenc\" + 0.009*\"feel\" + 0.008*\"day\" + 0.008*\"kind\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.062): 0.022*\"nan\" + 0.018*\"elizabeth\" + 0.013*\"realli\" + 0.012*\"know\" + 0.011*\"two\" + 0.011*\"feel\" + 0.011*\"move\" + 0.009*\"week\" + 0.008*\"get\" + 0.008*\"breath\"\n",
      "INFO:gensim.models.ldamodel:topic diff=7.312672, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=16, decay=0.5, chunksize=2000> in 0.23s', 'datetime': '2022-07-14T13:23:37.234174', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.058823529411764705\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.058823529411764705\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 17 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-12.682 per-word bound, 6570.6 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.059): 0.029*\"nan\" + 0.015*\"realli\" + 0.012*\"move\" + 0.012*\"elizabeth\" + 0.011*\"know\" + 0.010*\"two\" + 0.009*\"get\" + 0.009*\"feel\" + 0.009*\"week\" + 0.008*\"see\"\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.059): 0.016*\"elizabeth\" + 0.015*\"know\" + 0.013*\"realli\" + 0.009*\"like\" + 0.009*\"week\" + 0.008*\"call\" + 0.008*\"breath\" + 0.008*\"feel\" + 0.008*\"move\" + 0.007*\"see\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.059): 0.711*\"nan\" + 0.006*\"elizabeth\" + 0.003*\"know\" + 0.003*\"two\" + 0.003*\"breath\" + 0.003*\"day\" + 0.002*\"week\" + 0.002*\"wait\" + 0.002*\"feel\" + 0.002*\"agenc\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.059): 0.031*\"nan\" + 0.011*\"elizabeth\" + 0.008*\"breath\" + 0.008*\"two\" + 0.008*\"know\" + 0.008*\"oncologist\" + 0.007*\"much\" + 0.007*\"agenc\" + 0.007*\"feel\" + 0.006*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.059): 0.100*\"nan\" + 0.017*\"lindsey\" + 0.017*\"realli\" + 0.014*\"know\" + 0.014*\"maria\" + 0.014*\"like\" + 0.013*\"see\" + 0.012*\"could\" + 0.012*\"surgeri\" + 0.011*\"complet\"\n",
      "INFO:gensim.models.ldamodel:topic diff=7.706436, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=17, decay=0.5, chunksize=2000> in 0.22s', 'datetime': '2022-07-14T13:23:37.466830', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.05555555555555555\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.05555555555555555\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 18 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-13.134 per-word bound, 8988.6 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #11 (0.056): 0.011*\"lindsey\" + 0.011*\"know\" + 0.011*\"maria\" + 0.010*\"move\" + 0.010*\"see\" + 0.010*\"gone\" + 0.009*\"like\" + 0.009*\"breast\" + 0.009*\"realli\" + 0.007*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.056): 0.014*\"elizabeth\" + 0.010*\"know\" + 0.008*\"breath\" + 0.007*\"feel\" + 0.007*\"realli\" + 0.007*\"week\" + 0.006*\"nurs\" + 0.006*\"agenc\" + 0.006*\"day\" + 0.006*\"wait\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.056): 0.090*\"nan\" + 0.013*\"elizabeth\" + 0.009*\"breath\" + 0.009*\"know\" + 0.009*\"two\" + 0.008*\"much\" + 0.008*\"oncologist\" + 0.008*\"agenc\" + 0.007*\"feel\" + 0.007*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic #17 (0.056): 0.021*\"nan\" + 0.015*\"realli\" + 0.013*\"maria\" + 0.013*\"see\" + 0.013*\"like\" + 0.012*\"move\" + 0.012*\"call\" + 0.011*\"lindsey\" + 0.010*\"surgeri\" + 0.010*\"know\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.056): 0.733*\"nan\" + 0.005*\"elizabeth\" + 0.003*\"know\" + 0.002*\"day\" + 0.002*\"breath\" + 0.002*\"two\" + 0.002*\"week\" + 0.002*\"wait\" + 0.002*\"seat\" + 0.002*\"nurs\"\n",
      "INFO:gensim.models.ldamodel:topic diff=9.163173, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=18, decay=0.5, chunksize=2000> in 0.23s', 'datetime': '2022-07-14T13:23:37.709607', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.05263157894736842\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.05263157894736842\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 19 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-13.596 per-word bound, 12382.2 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #14 (0.053): 0.285*\"nan\" + 0.006*\"lindsey\" + 0.006*\"maria\" + 0.006*\"see\" + 0.005*\"know\" + 0.005*\"like\" + 0.005*\"surgeri\" + 0.005*\"realli\" + 0.005*\"move\" + 0.005*\"breast\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.053): 0.023*\"elizabeth\" + 0.023*\"nan\" + 0.010*\"breath\" + 0.010*\"know\" + 0.009*\"feel\" + 0.008*\"two\" + 0.008*\"wait\" + 0.007*\"nurs\" + 0.007*\"agenc\" + 0.007*\"kind\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.053): 0.054*\"nan\" + 0.018*\"realli\" + 0.017*\"lindsey\" + 0.015*\"like\" + 0.014*\"move\" + 0.013*\"see\" + 0.012*\"know\" + 0.012*\"call\" + 0.011*\"maria\" + 0.010*\"time\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.053): 0.859*\"nan\" + 0.001*\"elizabeth\" + 0.000*\"know\" + 0.000*\"breath\" + 0.000*\"two\" + 0.000*\"kind\" + 0.000*\"much\" + 0.000*\"agenc\" + 0.000*\"nurs\" + 0.000*\"day\"\n",
      "INFO:gensim.models.ldamodel:topic #11 (0.053): 0.010*\"lindsey\" + 0.010*\"maria\" + 0.010*\"know\" + 0.009*\"move\" + 0.009*\"see\" + 0.009*\"like\" + 0.009*\"gone\" + 0.009*\"breast\" + 0.008*\"realli\" + 0.007*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic diff=9.154127, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=19, decay=0.5, chunksize=2000> in 0.41s', 'datetime': '2022-07-14T13:23:38.133556', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.models.ldamodel:using symmetric alpha at 0.05\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.05\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 20 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-14.065 per-word bound, 17134.9 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.050): 0.646*\"nan\" + 0.005*\"realli\" + 0.005*\"know\" + 0.004*\"lindsey\" + 0.004*\"move\" + 0.004*\"get\" + 0.004*\"complet\" + 0.004*\"maria\" + 0.003*\"like\" + 0.003*\"breast\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.050): 0.023*\"elizabeth\" + 0.014*\"know\" + 0.014*\"feel\" + 0.013*\"two\" + 0.011*\"breath\" + 0.010*\"much\" + 0.010*\"realli\" + 0.010*\"week\" + 0.009*\"wait\" + 0.009*\"agenc\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.050): 0.024*\"elizabeth\" + 0.024*\"nan\" + 0.010*\"breath\" + 0.010*\"know\" + 0.008*\"feel\" + 0.008*\"two\" + 0.008*\"oncologist\" + 0.008*\"wait\" + 0.007*\"nurs\" + 0.007*\"kind\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.050): 0.139*\"nan\" + 0.008*\"realli\" + 0.007*\"elizabeth\" + 0.007*\"move\" + 0.006*\"know\" + 0.006*\"two\" + 0.005*\"feel\" + 0.005*\"get\" + 0.005*\"week\" + 0.005*\"see\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.050): 0.722*\"nan\" + 0.005*\"elizabeth\" + 0.003*\"know\" + 0.002*\"two\" + 0.002*\"breath\" + 0.002*\"day\" + 0.002*\"week\" + 0.002*\"wait\" + 0.002*\"feel\" + 0.002*\"nurs\"\n",
      "INFO:gensim.models.ldamodel:topic diff=9.595289, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=20, decay=0.5, chunksize=2000> in 0.22s', 'datetime': '2022-07-14T13:23:38.373928', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True)) #tokenization，return a list\n",
    "data_words = list(sent_to_words(text_random_20percent['processed'])) #tokenization\n",
    "bigram = gensim.models.Phrases(data_words,min_count=1,threshold=1)\n",
    "'''mincount：int, the times that two unigram co-occur must be equal or higher than this number，\n",
    "threshold：Phrases function will return a 'phrase score', it will decide whether two unigrams can be regarded as a bigram'''\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram) # Bigram\n",
    "def make_bigrams(texts): #Bigram\n",
    "    return [bigram[doc] for doc in texts]\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "texts = data_words_bigrams\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start,limit,step):\n",
    "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,random_state=2022)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "limit=21; start=10; step=1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values,label='20% random tweets')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(range(start,limit,step))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Coherence score **Umass**, we set the number of topics to **12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in the training set: 1300\n"
     ]
    }
   ],
   "source": [
    "print('The number of tweets in the training set:',len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Bigram LDA\n",
    "print out the top 50 bigrams in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.ldamodel:using autotuned alpha, starting with [0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336, 0.083333336]\n",
      "INFO:gensim.models.ldamodel:using symmetric eta at 0.08333333333333333\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 12 topics, 1 passes over the supplied corpus of 1300 documents, updating model once every 1300 documents, evaluating perplexity every 1300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:-10.518 per-word bound, 1466.2 perplexity estimate based on a held-out corpus of 1300 documents with 1809 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #1300/1300\n",
      "INFO:gensim.models.ldamodel:optimized alpha [0.07728599, 0.07657245, 0.07650697, 0.07659276, 0.08131927, 0.07678238, 0.07665067, 0.14546846, 0.07658926, 0.07658927, 0.08437517, 0.07658836]\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.077): 0.020*\"elizabeth\" + 0.009*\"know\" + 0.009*\"breath\" + 0.008*\"feel\" + 0.007*\"two\" + 0.007*\"nurs\" + 0.007*\"day\" + 0.007*\"wait\" + 0.007*\"oncologist\" + 0.006*\"agenc\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.077): 0.015*\"elizabeth\" + 0.012*\"know\" + 0.010*\"feel\" + 0.010*\"two\" + 0.010*\"realli\" + 0.008*\"week\" + 0.008*\"breath\" + 0.008*\"like\" + 0.008*\"move\" + 0.007*\"see\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.081): 0.600*\"nan\" + 0.007*\"elizabeth\" + 0.004*\"know\" + 0.003*\"two\" + 0.003*\"day\" + 0.003*\"breath\" + 0.003*\"week\" + 0.003*\"wait\" + 0.003*\"feel\" + 0.003*\"nurs\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.084): 0.720*\"nan\" + 0.003*\"know\" + 0.002*\"see\" + 0.002*\"realli\" + 0.002*\"like\" + 0.002*\"lindsey\" + 0.002*\"move\" + 0.002*\"maria\" + 0.002*\"breast\" + 0.002*\"call\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.145): 0.971*\"nan\" + 0.000*\"elizabeth\" + 0.000*\"feel\" + 0.000*\"know\" + 0.000*\"two\" + 0.000*\"nurs\" + 0.000*\"breath\" + 0.000*\"kind\" + 0.000*\"day\" + 0.000*\"week\"\n",
      "INFO:gensim.models.ldamodel:topic diff=4.736691, rho=1.000000\n",
      "INFO:gensim.utils:LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=315, num_topics=12, decay=0.5, chunksize=2000> in 0.24s', 'datetime': '2022-07-14T13:28:19.364576', 'gensim': '4.2.0', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: 0\n",
      "(job_line) (everi_singl) (think_track) (grown_overnight) (made_decis) (came_home) (someth_wrong) (seriou_relationship) (scare_parent) (know_like) (one_thing) (scene_wav) (elizabeth_ye) (sinc_last) (record_male) (breath_breath) (voic_take) (part_bodi) (nurs_tell) (pleas_breath) (breath_take) (huge_huge) (hold_pleas) (real_deep) (deep_breath) (breath_hold) (anoth_real) (take_anoth) \n",
      "Topic: 1\n",
      "(voic_take) (record_male) (elizabeth_ye) (part_bodi) (sinc_last) (breath_breath) (nurs_tell) (take_anoth) (anoth_real) (breath_hold) (one_thing) (hold_pleas) (real_deep) (deep_breath) (breath_take) (huge_huge) (know_like) (pleas_breath) (scene_wav) (everi_singl) (someth_wrong) (came_home) (scare_parent) (job_line) (think_track) (made_decis) (grown_overnight) (seriou_relationship) \n",
      "Topic: 2\n",
      "(breath_breath) (elizabeth_ye) (sinc_last) (nurs_tell) (voic_take) (part_bodi) (record_male) (breath_hold) (know_like) (huge_huge) (real_deep) (take_anoth) (breath_take) (one_thing) (anoth_real) (deep_breath) (scene_wav) (pleas_breath) (hold_pleas) (seriou_relationship) (came_home) (everi_singl) (someth_wrong) (job_line) (grown_overnight) (think_track) (made_decis) (scare_parent) \n",
      "Topic: 3\n",
      "(someth_wrong) (think_track) (made_decis) (job_line) (scare_parent) (everi_singl) (came_home) (grown_overnight) (seriou_relationship) (nurs_tell) (know_like) (scene_wav) (record_male) (voic_take) (elizabeth_ye) (sinc_last) (one_thing) (breath_breath) (part_bodi) (real_deep) (pleas_breath) (take_anoth) (huge_huge) (hold_pleas) (anoth_real) (breath_hold) (breath_take) (deep_breath) \n",
      "Topic: 4\n",
      "(part_bodi) (elizabeth_ye) (nurs_tell) (breath_breath) (voic_take) (sinc_last) (record_male) (real_deep) (know_like) (breath_take) (hold_pleas) (deep_breath) (scene_wav) (pleas_breath) (take_anoth) (breath_hold) (one_thing) (huge_huge) (anoth_real) (seriou_relationship) (scare_parent) (someth_wrong) (think_track) (job_line) (grown_overnight) (made_decis) (came_home) (everi_singl) \n",
      "Topic: 5\n",
      "(scare_parent) (job_line) (grown_overnight) (seriou_relationship) (everi_singl) (came_home) (made_decis) (someth_wrong) (one_thing) (think_track) (know_like) (nurs_tell) (scene_wav) (record_male) (part_bodi) (breath_breath) (sinc_last) (voic_take) (elizabeth_ye) (huge_huge) (breath_hold) (pleas_breath) (anoth_real) (take_anoth) (hold_pleas) (breath_take) (deep_breath) (real_deep) \n",
      "Topic: 6\n",
      "(sinc_last) (elizabeth_ye) (part_bodi) (breath_breath) (voic_take) (scene_wav) (nurs_tell) (know_like) (record_male) (one_thing) (deep_breath) (hold_pleas) (breath_take) (someth_wrong) (take_anoth) (seriou_relationship) (breath_hold) (everi_singl) (pleas_breath) (real_deep) (job_line) (huge_huge) (scare_parent) (think_track) (grown_overnight) (anoth_real) (came_home) (made_decis) \n",
      "Topic: 7\n",
      "(sinc_last) (breath_breath) (part_bodi) (voic_take) (record_male) (elizabeth_ye) (nurs_tell) (hold_pleas) (deep_breath) (breath_hold) (take_anoth) (know_like) (real_deep) (one_thing) (huge_huge) (anoth_real) (pleas_breath) (breath_take) (scene_wav) (everi_singl) (seriou_relationship) (someth_wrong) (think_track) (job_line) (made_decis) (grown_overnight) (scare_parent) (came_home) \n",
      "Topic: 8\n",
      "(breath_breath) (voic_take) (record_male) (sinc_last) (part_bodi) (elizabeth_ye) (scene_wav) (one_thing) (nurs_tell) (someth_wrong) (anoth_real) (everi_singl) (deep_breath) (grown_overnight) (pleas_breath) (know_like) (hold_pleas) (real_deep) (scare_parent) (huge_huge) (made_decis) (breath_take) (came_home) (seriou_relationship) (breath_hold) (take_anoth) (job_line) (think_track) \n",
      "Topic: 9\n",
      "(seriou_relationship) (came_home) (everi_singl) (think_track) (grown_overnight) (made_decis) (scare_parent) (someth_wrong) (record_male) (job_line) (scene_wav) (one_thing) (voic_take) (elizabeth_ye) (part_bodi) (sinc_last) (know_like) (nurs_tell) (breath_breath) (huge_huge) (take_anoth) (hold_pleas) (breath_take) (breath_hold) (real_deep) (pleas_breath) (anoth_real) (deep_breath) \n",
      "Topic: 10\n",
      "(think_track) (came_home) (job_line) (someth_wrong) (know_like) (seriou_relationship) (made_decis) (scare_parent) (everi_singl) (one_thing) (grown_overnight) (scene_wav) (nurs_tell) (breath_breath) (record_male) (anoth_real) (voic_take) (sinc_last) (elizabeth_ye) (part_bodi) (take_anoth) (real_deep) (deep_breath) (pleas_breath) (huge_huge) (breath_hold) (hold_pleas) (breath_take) \n",
      "Topic: 11\n",
      "(made_decis) (think_track) (grown_overnight) (seriou_relationship) (scare_parent) (came_home) (everi_singl) (one_thing) (job_line) (scene_wav) (someth_wrong) (know_like) (nurs_tell) (record_male) (breath_breath) (elizabeth_ye) (part_bodi) (voic_take) (sinc_last) (pleas_breath) (anoth_real) (deep_breath) (breath_take) (breath_hold) (real_deep) (hold_pleas) (huge_huge) (take_anoth) "
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=12,random_state=2022,alpha='auto',per_word_topics=True)\n",
    "\n",
    "amount =0\n",
    "for idx, topic in lda_model.show_topics(formatted=False,num_topics=12,num_words= 500):\n",
    "    print('\\n',end='')\n",
    "    print('Topic:',idx)\n",
    "    num = 0\n",
    "    for w in topic:\n",
    "        if '_' in w[0] and num < 50:\n",
    "            #print(round(w[1],4),'*',w[0],', ',end='',sep='')\n",
    "            print('(',w[0],') ',end='',sep='')\n",
    "            num += 1 \n",
    "            amount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GuidedLDA Model\n",
    "latent topics are identified from the bigrams shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 6501\n",
      "INFO:lda:vocab_size: 733\n",
      "INFO:lda:n_words: 8737\n",
      "INFO:lda:n_topics: 4\n",
      "INFO:lda:n_iter: 1000\n",
      "INFO:lda:<0> log likelihood: -34968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe  skipped\n",
      "trust  skipped\n",
      "church  skipped\n",
      "conviction  skipped\n",
      "optimism  skipped\n",
      "religion  skipped\n",
      "ideaology  skipped\n",
      "confidence  skipped\n",
      "mass  skipped\n",
      "synagogue  skipped\n",
      "anticipate  skipped\n",
      "future  skipped\n",
      "lord  skipped\n",
      "ready  skipped\n",
      "focus  skipped\n",
      "positive  skipped\n",
      "desire  skipped\n",
      "acceptance  skipped\n",
      "loyal  skipped\n",
      "truth  skipped\n",
      "allegiance  skipped\n",
      "assent  skipped\n",
      "assurance  skipped\n",
      "certain  skipped\n",
      "constant  skipped\n",
      "credence  skipped\n",
      "credit  skipped\n",
      "credulity  skipped\n",
      "depend  skipped\n",
      "fealty  skipped\n",
      "fidelity  skipped\n",
      "reliance  skipped\n",
      "stock  skipped\n",
      "store  skipped\n",
      "sureness  skipped\n",
      "surety  skipped\n",
      "troth  skipped\n",
      "radiology  skipped\n",
      "chemotherapy  skipped\n",
      "benign  skipped\n",
      "invasive  skipped\n",
      "masectomy  skipped\n",
      "surgery  skipped\n",
      "malignant  skipped\n",
      "metastasis  skipped\n",
      "melanoma  skipped\n",
      "carcigen  skipped\n",
      "cancerous  skipped\n",
      "precancerous  skipped\n",
      "survivor  skipped\n",
      "disease  skipped\n",
      "sick  skipped\n",
      "ill  skipped\n",
      "spread  skipped\n",
      "lump  skipped\n",
      "cell  skipped\n",
      "body  skipped\n",
      "organ  skipped\n",
      "lungs  skipped\n",
      "heal  skipped\n",
      "emergency  skipped\n",
      "prescription  skipped\n",
      "m.d.  skipped\n",
      "passed  skipped\n",
      "prognosis  skipped\n",
      "migrane  skipped\n",
      "seizure  skipped\n",
      "c.t.  skipped\n",
      "imaging  skipped\n",
      "diagnosis  skipped\n",
      "biopsy  skipped\n",
      "removed  skipped\n",
      "corruption  skipped\n",
      "carcinoma  skipped\n",
      "big c  skipped\n",
      "sky  skipped\n",
      "stars  skipped\n",
      "trees  skipped\n",
      "flowers  skipped\n",
      "sea  skipped\n",
      "ocean  skipped\n",
      "lake  skipped\n",
      "storm  skipped\n",
      "thunder  skipped\n",
      "lightening  skipped\n",
      "snow  skipped\n",
      "sunset  skipped\n",
      "sunrise  skipped\n",
      "outside  skipped\n",
      "leaf  skipped\n",
      "trek  skipped\n",
      "wilderness  skipped\n",
      "animal  skipped\n",
      "stream  skipped\n",
      "bolt  skipped\n",
      "weather  skipped\n",
      "hurricane  skipped\n",
      "mold  skipped\n",
      "morning  skipped\n",
      "environment  skipped\n",
      "landscape  skipped\n",
      "view  skipped\n",
      "cosmos  skipped\n",
      "country  skipped\n",
      "generation  skipped\n",
      "macrocosm  skipped\n",
      "scenery  skipped\n",
      "seascape  skipped\n",
      "setting  skipped\n",
      "universe  skipped\n",
      "natural  skipped\n",
      "reflection  skipped\n",
      "self care  skipped\n",
      "caution  skipped\n",
      "consideration  skipped\n",
      "conscientious  skipped\n",
      "regard  skipped\n",
      "thougt  skipped\n",
      "wellness  skipped\n",
      "well-being  skipped\n",
      "feelings  skipped\n",
      "mental health  skipped\n",
      "maintenance  skipped\n",
      "introspection  skipped\n",
      "center  skipped\n",
      "concentrate  skipped\n",
      "relax  skipped\n",
      "focus  skipped\n",
      "listen  skipped\n",
      "yoga  skipped\n",
      "body  skipped\n",
      "hardships  skipped\n",
      "redifine  skipped\n",
      "experience  skipped\n",
      "tough  skipped\n",
      "heal  skipped\n",
      "nerve  skipped\n",
      "positive  skipped\n",
      "heavy  skipped\n",
      "activity  skipped\n",
      "headache  skipped\n",
      "body  skipped\n",
      "sense  skipped\n",
      "pause  skipped\n",
      "notice  skipped\n",
      "depression  skipped\n",
      "self-esteem  skipped\n",
      "confident  skipped\n",
      "alertness  skipped\n",
      "carefulness  skipped\n",
      "circumspection  skipped\n",
      "concern  skipped\n",
      "diligence  skipped\n",
      "direction  skipped\n",
      "discrimination  skipped\n",
      "effort  skipped\n",
      "enthusiasm  skipped\n",
      "exactness  skipped\n",
      "exertion  skipped\n",
      "fastidiousness  skipped\n",
      "forethougt  skipped\n",
      "head  skipped\n",
      "heedfulness  skipped\n",
      "interest  skipped\n",
      "management  skipped\n",
      "meticulousness  skipped\n",
      "nicety  skipped\n",
      "pain  skipped\n",
      "particularity  skipped\n",
      "precaution  skipped\n",
      "prudence  skipped\n",
      "regard  skipped\n",
      "scrupulousness  skipped\n",
      "solicitude  skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<10> log likelihood: -30256\n",
      "INFO:lda:<20> log likelihood: -29379\n",
      "INFO:lda:<30> log likelihood: -28737\n",
      "INFO:lda:<40> log likelihood: -27996\n",
      "INFO:lda:<50> log likelihood: -27590\n",
      "INFO:lda:<60> log likelihood: -26466\n",
      "INFO:lda:<70> log likelihood: -26234\n",
      "INFO:lda:<80> log likelihood: -26136\n",
      "INFO:lda:<90> log likelihood: -25905\n",
      "INFO:lda:<100> log likelihood: -25928\n",
      "INFO:lda:<110> log likelihood: -25865\n",
      "INFO:lda:<120> log likelihood: -25884\n",
      "INFO:lda:<130> log likelihood: -25865\n",
      "INFO:lda:<140> log likelihood: -25865\n",
      "INFO:lda:<150> log likelihood: -25865\n",
      "INFO:lda:<160> log likelihood: -25865\n",
      "INFO:lda:<170> log likelihood: -25865\n",
      "INFO:lda:<180> log likelihood: -25865\n",
      "INFO:lda:<190> log likelihood: -25865\n",
      "INFO:lda:<200> log likelihood: -25865\n",
      "INFO:lda:<210> log likelihood: -25865\n",
      "INFO:lda:<220> log likelihood: -25865\n",
      "INFO:lda:<230> log likelihood: -25865\n",
      "INFO:lda:<240> log likelihood: -25865\n",
      "INFO:lda:<250> log likelihood: -25865\n",
      "INFO:lda:<260> log likelihood: -25865\n",
      "INFO:lda:<270> log likelihood: -25865\n",
      "INFO:lda:<280> log likelihood: -25865\n",
      "INFO:lda:<290> log likelihood: -25865\n",
      "INFO:lda:<300> log likelihood: -25865\n",
      "INFO:lda:<310> log likelihood: -25865\n",
      "INFO:lda:<320> log likelihood: -25865\n",
      "INFO:lda:<330> log likelihood: -25865\n",
      "INFO:lda:<340> log likelihood: -25865\n",
      "INFO:lda:<350> log likelihood: -25865\n",
      "INFO:lda:<360> log likelihood: -25865\n",
      "INFO:lda:<370> log likelihood: -25865\n",
      "INFO:lda:<380> log likelihood: -25865\n",
      "INFO:lda:<390> log likelihood: -25865\n",
      "INFO:lda:<400> log likelihood: -25865\n",
      "INFO:lda:<410> log likelihood: -25865\n",
      "INFO:lda:<420> log likelihood: -25865\n",
      "INFO:lda:<430> log likelihood: -25865\n",
      "INFO:lda:<440> log likelihood: -25865\n",
      "INFO:lda:<450> log likelihood: -25865\n",
      "INFO:lda:<460> log likelihood: -25865\n",
      "INFO:lda:<470> log likelihood: -25865\n",
      "INFO:lda:<480> log likelihood: -25865\n",
      "INFO:lda:<490> log likelihood: -25865\n",
      "INFO:lda:<500> log likelihood: -25865\n",
      "INFO:lda:<510> log likelihood: -25865\n",
      "INFO:lda:<520> log likelihood: -25865\n",
      "INFO:lda:<530> log likelihood: -25865\n",
      "INFO:lda:<540> log likelihood: -25874\n",
      "INFO:lda:<550> log likelihood: -25865\n",
      "INFO:lda:<560> log likelihood: -25865\n",
      "INFO:lda:<570> log likelihood: -25865\n",
      "INFO:lda:<580> log likelihood: -25865\n",
      "INFO:lda:<590> log likelihood: -25865\n",
      "INFO:lda:<600> log likelihood: -25865\n",
      "INFO:lda:<610> log likelihood: -25873\n",
      "INFO:lda:<620> log likelihood: -25865\n",
      "INFO:lda:<630> log likelihood: -25865\n",
      "INFO:lda:<640> log likelihood: -25865\n",
      "INFO:lda:<650> log likelihood: -25865\n",
      "INFO:lda:<660> log likelihood: -25865\n",
      "INFO:lda:<670> log likelihood: -25865\n",
      "INFO:lda:<680> log likelihood: -25865\n",
      "INFO:lda:<690> log likelihood: -25865\n",
      "INFO:lda:<700> log likelihood: -25865\n",
      "INFO:lda:<710> log likelihood: -25865\n",
      "INFO:lda:<720> log likelihood: -25865\n",
      "INFO:lda:<730> log likelihood: -25865\n",
      "INFO:lda:<740> log likelihood: -25865\n",
      "INFO:lda:<750> log likelihood: -25865\n",
      "INFO:lda:<760> log likelihood: -25865\n",
      "INFO:lda:<770> log likelihood: -25865\n",
      "INFO:lda:<780> log likelihood: -25865\n",
      "INFO:lda:<790> log likelihood: -25865\n",
      "INFO:lda:<800> log likelihood: -25865\n",
      "INFO:lda:<810> log likelihood: -25875\n",
      "INFO:lda:<820> log likelihood: -25865\n",
      "INFO:lda:<830> log likelihood: -25865\n",
      "INFO:lda:<840> log likelihood: -25865\n",
      "INFO:lda:<850> log likelihood: -25865\n",
      "INFO:lda:<860> log likelihood: -25865\n",
      "INFO:lda:<870> log likelihood: -25865\n",
      "INFO:lda:<880> log likelihood: -25865\n",
      "INFO:lda:<890> log likelihood: -25865\n",
      "INFO:lda:<900> log likelihood: -25865\n",
      "INFO:lda:<910> log likelihood: -25865\n",
      "INFO:lda:<920> log likelihood: -25865\n",
      "INFO:lda:<930> log likelihood: -25865\n",
      "INFO:lda:<940> log likelihood: -25865\n",
      "INFO:lda:<950> log likelihood: -25865\n",
      "INFO:lda:<960> log likelihood: -25865\n",
      "INFO:lda:<970> log likelihood: -25865\n",
      "INFO:lda:<980> log likelihood: -25865\n",
      "INFO:lda:<990> log likelihood: -25865\n",
      "INFO:lda:<999> log likelihood: -25875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic: 0\n",
      "0.9966*nan  0.0*young  0.0*give  0.0*gosh  0.0*good  0.0*gonna  0.0*gone  0.0*goe  0.0*god  0.0*goat  0.0*go  0.0*given  0.0*get  0.0*freak  0.0*gener  0.0*gear  0.0*futur  0.0*fun  0.0*full  0.0*fulfil  \n",
      "\n",
      "Topic: 1\n",
      "0.9967*nan  0.0*young  0.0*give  0.0*gosh  0.0*good  0.0*gonna  0.0*gone  0.0*goe  0.0*god  0.0*goat  0.0*go  0.0*given  0.0*get  0.0*freak  0.0*gener  0.0*gear  0.0*futur  0.0*fun  0.0*full  0.0*fulfil  \n",
      "\n",
      "Topic: 2\n",
      "0.9961*nan  0.0005*tape  0.0*young  0.0*get  0.0*gonna  0.0*gone  0.0*goe  0.0*god  0.0*goat  0.0*go  0.0*given  0.0*give  0.0*gear  0.0*gener  0.0*gosh  0.0*futur  0.0*fun  0.0*full  0.0*fulfil  0.0*fuck  \n",
      "\n",
      "Topic: 3\n",
      "0.0173*right  0.0168*like  0.0124*yeah  0.0124*lindsey  0.0124*know  0.0111*one  0.0106*elizabeth  0.0106*lar  0.0098*come  0.0093*realli  0.0084*see  0.008*would  0.0071*get  0.0071*think  0.0071*year  0.0071*feel  0.0067*deni  0.0067*time  0.0067*cancer  0.0067*guid  "
     ]
    }
   ],
   "source": [
    "bigramseed = [['god','hope','pray','believe','trust','church','conviction','optimism','religion',\n",
    "               'ideaology','confidence','group','relationship','mass','synagogue','anticipate',\n",
    "               'future','strong','want','lord','ready','wait','focus','positive','plan','scare',\n",
    "               'support','desire','acceptance','loyal','truth','allegiance','assent','assurance',\n",
    "               'certain','constant','credence','credit','credulity','depend','fealty','fidelity',\n",
    "               'reliance','stock','store','sureness','surety','troth'],#0 Faith\n",
    "             ['tumor','radiology','chemotherapy','benign','invasive','masectomy','surgery',\n",
    "              'malignant','metastasis','melanoma','carcigen','cancerous','precancerous',\n",
    "              'survivor','disease','sick','ill','spread','lump','cell','grow','neck','body',\n",
    "             'organ','lungs','heal','emergency','oncologist','prescription','m.d.','test','passed',\n",
    "             'prognosis','migrane','seizure','c.t.','scan','imaging','heartbeat','diagnosis','biopsy',\n",
    "             'removed','corruption','carcinoma','big c'],#1 cancer\n",
    "             ['earth','sun','sky','stars','trees','flowers','land','sea','ocean','lake','river','rain',\n",
    "             'storm','thunder','lightening','snow','sunset','sunrise','outside','leaf','trek','wilderness',\n",
    "              'animal','stream','rock','bolt','flood','weather','water','hurricane','mold','morning','night',\n",
    "              'shellfish','ground','world','trash','life','track','environment','landscape','view','cosmos',\n",
    "              'country','forest','generation','macrocosm','outdoor','scenery','seascape','setting','universe',\n",
    "              'natural',],#2 Nature\n",
    "             ['reflection','care','self care','health','caution','consideration',\n",
    "              'conscientious','regard','thought','thougt','wellness','well-being','feelings','mental health','maintenance','introspection',\n",
    "             'center','concentrate','breath','relax','sit','focus','ground','listen','yoga','body','hardships','redifine','strong','experience',\n",
    "             'tough','sign','know','heal','sleep','nerve','positive','mind','heavy','activity','headache','deep','body','sense','tension','pause',\n",
    "              'notice','heartbeat','move','forward','weight','depression','heart','check','support','self-esteem',\n",
    "             'confident','alertness','carefulness','circumspection','concern','diligence','direction','discrimination','effort','enthusiasm',\n",
    "             'exactness','exertion','fastidiousness','forethougt','head','heedfulness','interest','management','meticulousness','nicety','pain',\n",
    "              'particularity','precaution','prudence','regard','scrupulousness','solicitude']]#3 Mindfullness\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lda import guidedlda as guidedlda\n",
    "import numpy as np\n",
    "model = guidedlda.GuidedLDA(n_topics=4,n_iter=1000,random_state=2022,refresh=10,alpha=0.01,eta=0.01)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text2['processed'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(bigramseed):\n",
    "    for word in st:\n",
    "        try:\n",
    "            seed_topics[word2id[word]] = t_id\n",
    "        except:\n",
    "            print(word,\" skipped\")\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.7) #set seed confidence to 0.7\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 20\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word): #Print out results\n",
    "    print('\\n')\n",
    "    print('Topic:',i)\n",
    "    words_probability = np.array(-topic_dist)\n",
    "    for index in range(n_top_words):\n",
    "        print(round(abs(np.sort(words_probability))[:(n_top_words)][index],4),'*',\n",
    "              np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1][index],sep='',end='  ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{244: 0,\n",
       " 295: 0,\n",
       " 482: 0,\n",
       " 261: 0,\n",
       " 513: 0,\n",
       " 617: 3,\n",
       " 693: 0,\n",
       " 690: 0,\n",
       " 469: 0,\n",
       " 542: 0,\n",
       " 623: 3,\n",
       " 674: 1,\n",
       " 262: 1,\n",
       " 407: 1,\n",
       " 433: 1,\n",
       " 642: 1,\n",
       " 540: 1,\n",
       " 277: 3,\n",
       " 154: 2,\n",
       " 621: 2,\n",
       " 328: 2,\n",
       " 527: 2,\n",
       " 500: 2,\n",
       " 528: 2,\n",
       " 214: 2,\n",
       " 694: 2,\n",
       " 416: 2,\n",
       " 566: 2,\n",
       " 260: 3,\n",
       " 718: 2,\n",
       " 667: 2,\n",
       " 341: 2,\n",
       " 665: 2,\n",
       " 220: 2,\n",
       " 440: 2,\n",
       " 91: 3,\n",
       " 273: 3,\n",
       " 645: 3,\n",
       " 72: 3,\n",
       " 582: 3,\n",
       " 574: 3,\n",
       " 326: 3,\n",
       " 586: 3,\n",
       " 390: 3,\n",
       " 133: 3,\n",
       " 638: 3,\n",
       " 399: 3,\n",
       " 225: 3,\n",
       " 700: 3,\n",
       " 276: 3,\n",
       " 97: 3}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(enumerate(bigramseed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'God'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_44908/3673544624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'God'"
     ]
    }
   ],
   "source": [
    "word2id[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ['foodtech', 'cellular', 'medium', 'stem', 'dprint', 'bioprint', 'cellag']), (1, ['seafood', 'sushi', 'nigiri', 'fish', 'shrimp', 'crab', 'aquacultur', 'lobster', 'salmon']), (2, ['steak', 'poultri', 'beef', 'chicken', 'duck']), (3, ['climat', 'chang', 'carbon', 'sustain', 'sustainablefood', 'sustainablemeat', 'feed', 'serv', 'demand']), (4, ['altprotein', 'altern', 'protein', 'alt', 'plant', 'plantbas', 'vegan']), (5, ['slaughter', 'cruelti', 'harm', 'anim', 'kill', 'welfar']), (6, ['healthi', 'healthier', 'health', 'fat']), (7, ['market', 'product', 'industri', 'suppli', 'chain', 'commerci']), (8, ['invest', 'startup', 'round', 'seed', 'fund', 'backer']), (9, ['demo', 'event', 'speak', 'host', 'festiv', 'symposium', 'join', 'talk', 'goodfoodconfer', 'watch', 'live', 'podcast', 'news', 'read', 'blog', 'post', 'check', 'interview', 'articl', 'thank', 'announc', 'discuss']), (10, ['join', 'team', 'hire']), (11, ['fda', 'usda', 'approv', 'advoc'])]\n"
     ]
    }
   ],
   "source": [
    "print (list(enumerate(bigramseed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prominent Factor\n",
    "For any tweet, let *t1* be the topic with the highest probability (*p1*) and the *t2* the next ranking topic (with probability *p2*), as determined by performing Guided LDA on the entire dadaset. We call the ratio of *p1*/*p2* the 'Prominent Factor' or PF. If *t1* is much more prominent than '*t2*', PF will be high. A factor higher than 1.4 means that one topic is relatively predominant for this tweet.\n",
    "\n",
    "*References:*\n",
    "\n",
    "Nugroho, Robertus, et al. \"Incorporating tweet relationships into topic derivation.\" Conference of the Pacific Association for Computational Linguistics. Springer, Singapore, 2015.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prominent factor:0.22%\n"
     ]
    }
   ],
   "source": [
    "prominent_factor = []\n",
    "doc_topic = model.transform(X)\n",
    "for i in range(len(doc_topic)):\n",
    "    prominent_factor.append(sorted(doc_topic[i])[-1]/sorted(doc_topic[i])[-2])\n",
    "count = 0\n",
    "for i in prominent_factor:\n",
    "    if i >= 1.4:\n",
    "        count+=1\n",
    "print('Prominent factor:',round(count/len(text)*100,2),'%',sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (14) does not match length of index (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_35723/2652209250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnumber_of_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic_number'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnumber_of_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proportion'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'%'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumber_of_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m number_of_tweets['Topic'] = ['Manufacturing process','Seafood','Meat product','Sustainability',\n\u001b[0m\u001b[1;32m     13\u001b[0m                                  \u001b[0;34m'Alternative protein'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Animal welfare'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Health and nutrition'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Industry and market'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                  \u001b[0;34m'Fundraising'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Event promotion and media release'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Hiring information'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (14) does not match length of index (2)"
     ]
    }
   ],
   "source": [
    "topic_number = []\n",
    "number = []\n",
    "topic_probability = []\n",
    "for i in range(len(doc_topic)):\n",
    "    topic_number.append(doc_topic[i].argmax())\n",
    "    topic_probability.append(doc_topic[i][doc_topic[i].argmax()])\n",
    "    number.append('1')\n",
    "data = pd.DataFrame(data=[i for i in topic_number],columns=['topic_number'])\n",
    "data['number'] = [i for i in number]\n",
    "number_of_tweets = pd.DataFrame(data.groupby('topic_number')['number'].count())\n",
    "number_of_tweets['proportion'] = [str(round(i/len(text)*100,2))+'%' for i in number_of_tweets['number']]\n",
    "number_of_tweets['Topic'] = ['Manufacturing process','Seafood','Meat product','Sustainability',\n",
    "                                 'Alternative protein','Animal welfare','Health and nutrition','Industry and market',\n",
    "                                 'Fundraising','Event promotion and media release','Hiring information',\n",
    "                                 'Regulation','Unseeded topic1','Unseeded topic2']\n",
    "number_of_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company-Topic Heatmap\n",
    "Based on proportion of each topic for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = model.transform(X)\n",
    "topic_number1 = []\n",
    "\n",
    "\n",
    "for i in range(len(doc_topic)):\n",
    "    topic_number1.append(doc_topic[i].argmax())\n",
    "text2['topic number'] = [i for i in topic_number1]\n",
    "# topic_author = text2.groupby(['topic number','Company'])['tweets'].count()\n",
    "# topic_author_3d = topic_author.unstack()\n",
    "# topic_author_3d = topic_author_3d[['Memphis Meats','biftek.co 🔬👩‍🔬🐄🥗','Aleph Farms','SuperMeat',\n",
    "#                                   'Finless Foods','shiokmeats','BlueNalu','New Age Meats','CUBIQ FOODS',\n",
    "#                                   'Mosa Meat','Wildtype','Meatable','Future Fields','Vow',\n",
    "#                                   'FutureMeat','Balletic Foods','LabFarmFoods','Avant Meats','Mission Barns']]\n",
    "# topic = ['Manufacturing process and supplies','Seafood product','Meat product','Sustainability',\n",
    "#         'Animal welfare','Alternative protein','Health and nutrition','Regulation','Industry and market','Fundraising',\n",
    "#         'Hiring Information','Event promotion and media release','Unseeded topic 1','Unseeded topic 2']\n",
    "\n",
    "# company = ['UPSIDE Foods','Biftek.co','Aleph Farms','SuperMeat','Fineless Foods','Shiok Meats','BlueNalu',\n",
    "#           'New Age Meats','Cubiq Foods','Mosa Meat','Wild Type','Meatable','Future Fields','Vow','Future Meat',\n",
    "#            'Balletic Foods','Lab Farm Foods','Avant Meats','Mission Barns']\n",
    "# topic_author_3d = topic_author_3d.fillna(0)#replace NaN by 0\n",
    "# topic_author_3d = topic_author_3d.reindex([0,1,2,3,5,4,6,11,7,8,10,9,12,13])# reindex\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(11,8))\n",
    "# plt.imshow(topic_author_3d.div(topic_author_3d.sum(axis=0),axis=1),cmap=\"Blues\")\n",
    "# plt.colorbar().ax.set_ylabel('Proportion of each topic for each company')\n",
    "# plt.xticks(range(len(company)), company,rotation=90)\n",
    "# plt.yticks(range(len(topic)), topic)\n",
    "# plt.xlabel('Company')\n",
    "# plt.ylabel('Topic')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_number1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "      <th>Company</th>\n",
       "      <th>url</th>\n",
       "      <th>processed</th>\n",
       "      <th>topic number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>1/5/21 15:59</td>\n",
       "      <td>transcript 1</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134648...</td>\n",
       "      <td>scene lar lar know bag everyth yeah yeah one t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>scene_2.wav\\nElizabeth: [00:00:01] This is my ...</td>\n",
       "      <td>12/30/20 23:14</td>\n",
       "      <td>transcript 2</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134442...</td>\n",
       "      <td>scene wav elizabeth case run realli fast away ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>scene_3.wav\\nLindsey: [00:00:03] Before I was ...</td>\n",
       "      <td>12/23/20 19:00</td>\n",
       "      <td>transcript 3</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134182...</td>\n",
       "      <td>scene wav lindsey diagnos still colleg found t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>12/16/20 21:26</td>\n",
       "      <td>transcript 4</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133932...</td>\n",
       "      <td>scene lar lar know bag everyth yeah yeah one t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...</td>\n",
       "      <td>12/15/20 20:16</td>\n",
       "      <td>transcript 5</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133894...</td>\n",
       "      <td>scene elizabeth saw hematologist oncologist fi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6501 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             tweets  \\\n",
       "0            0.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "1            1.0  scene_2.wav\\nElizabeth: [00:00:01] This is my ...   \n",
       "2            2.0  scene_3.wav\\nLindsey: [00:00:03] Before I was ...   \n",
       "3            4.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "4            5.0  Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...   \n",
       "...          ...                                                ...   \n",
       "6496         NaN                                                NaN   \n",
       "6497         NaN                                                NaN   \n",
       "6498         NaN                                                NaN   \n",
       "6499         NaN                                                NaN   \n",
       "6500         NaN                                                NaN   \n",
       "\n",
       "                date       Company  \\\n",
       "0       1/5/21 15:59  transcript 1   \n",
       "1     12/30/20 23:14  transcript 2   \n",
       "2     12/23/20 19:00  transcript 3   \n",
       "3     12/16/20 21:26  transcript 4   \n",
       "4     12/15/20 20:16  transcript 5   \n",
       "...              ...           ...   \n",
       "6496             NaN           NaN   \n",
       "6497             NaN           NaN   \n",
       "6498             NaN           NaN   \n",
       "6499             NaN           NaN   \n",
       "6500             NaN           NaN   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://twitter.com/MemphisMeats/status/134648...   \n",
       "1     https://twitter.com/MemphisMeats/status/134442...   \n",
       "2     https://twitter.com/MemphisMeats/status/134182...   \n",
       "3     https://twitter.com/MemphisMeats/status/133932...   \n",
       "4     https://twitter.com/MemphisMeats/status/133894...   \n",
       "...                                                 ...   \n",
       "6496                                                NaN   \n",
       "6497                                                NaN   \n",
       "6498                                                NaN   \n",
       "6499                                                NaN   \n",
       "6500                                                NaN   \n",
       "\n",
       "                                              processed  topic number  \n",
       "0     scene lar lar know bag everyth yeah yeah one t...             3  \n",
       "1     scene wav elizabeth case run realli fast away ...             3  \n",
       "2     scene wav lindsey diagnos still colleg found t...             3  \n",
       "3     scene lar lar know bag everyth yeah yeah one t...             3  \n",
       "4     scene elizabeth saw hematologist oncologist fi...             3  \n",
       "...                                                 ...           ...  \n",
       "6496                                               nan              1  \n",
       "6497                                               nan              1  \n",
       "6498                                               nan              1  \n",
       "6499                                               nan              1  \n",
       "6500                                               nan              1  \n",
       "\n",
       "[6501 rows x 7 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "      <th>Company</th>\n",
       "      <th>url</th>\n",
       "      <th>processed</th>\n",
       "      <th>topic number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>1/5/21 15:59</td>\n",
       "      <td>transcript 1</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134648...</td>\n",
       "      <td>scene lar lar know bag everyth yeah yeah one t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>scene_2.wav\\nElizabeth: [00:00:01] This is my ...</td>\n",
       "      <td>12/30/20 23:14</td>\n",
       "      <td>transcript 2</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134442...</td>\n",
       "      <td>scene wav elizabeth case run realli fast away ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>scene_3.wav\\nLindsey: [00:00:03] Before I was ...</td>\n",
       "      <td>12/23/20 19:00</td>\n",
       "      <td>transcript 3</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/134182...</td>\n",
       "      <td>scene wav lindsey diagnos still colleg found t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...</td>\n",
       "      <td>12/16/20 21:26</td>\n",
       "      <td>transcript 4</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133932...</td>\n",
       "      <td>scene lar lar know bag everyth yeah yeah one t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...</td>\n",
       "      <td>12/15/20 20:16</td>\n",
       "      <td>transcript 5</td>\n",
       "      <td>https://twitter.com/MemphisMeats/status/133894...</td>\n",
       "      <td>scene elizabeth saw hematologist oncologist fi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6498</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6501 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             tweets  \\\n",
       "0            0.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "1            1.0  scene_2.wav\\nElizabeth: [00:00:01] This is my ...   \n",
       "2            2.0  scene_3.wav\\nLindsey: [00:00:03] Before I was ...   \n",
       "3            4.0  Scene 4\\n\\nLars: [00:15:35] 35. [00:15:30]\\n\\n...   \n",
       "4            5.0  Scene 5\\n\\nElizabeth: [00:21:32] So [00:21:30]...   \n",
       "...          ...                                                ...   \n",
       "6496         NaN                                                NaN   \n",
       "6497         NaN                                                NaN   \n",
       "6498         NaN                                                NaN   \n",
       "6499         NaN                                                NaN   \n",
       "6500         NaN                                                NaN   \n",
       "\n",
       "                date       Company  \\\n",
       "0       1/5/21 15:59  transcript 1   \n",
       "1     12/30/20 23:14  transcript 2   \n",
       "2     12/23/20 19:00  transcript 3   \n",
       "3     12/16/20 21:26  transcript 4   \n",
       "4     12/15/20 20:16  transcript 5   \n",
       "...              ...           ...   \n",
       "6496             NaN           NaN   \n",
       "6497             NaN           NaN   \n",
       "6498             NaN           NaN   \n",
       "6499             NaN           NaN   \n",
       "6500             NaN           NaN   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://twitter.com/MemphisMeats/status/134648...   \n",
       "1     https://twitter.com/MemphisMeats/status/134442...   \n",
       "2     https://twitter.com/MemphisMeats/status/134182...   \n",
       "3     https://twitter.com/MemphisMeats/status/133932...   \n",
       "4     https://twitter.com/MemphisMeats/status/133894...   \n",
       "...                                                 ...   \n",
       "6496                                                NaN   \n",
       "6497                                                NaN   \n",
       "6498                                                NaN   \n",
       "6499                                                NaN   \n",
       "6500                                                NaN   \n",
       "\n",
       "                                              processed  topic number  \n",
       "0     scene lar lar know bag everyth yeah yeah one t...             3  \n",
       "1     scene wav elizabeth case run realli fast away ...             3  \n",
       "2     scene wav lindsey diagnos still colleg found t...             3  \n",
       "3     scene lar lar know bag everyth yeah yeah one t...             3  \n",
       "4     scene elizabeth saw hematologist oncologist fi...             3  \n",
       "...                                                 ...           ...  \n",
       "6496                                               nan              1  \n",
       "6497                                               nan              1  \n",
       "6498                                               nan              1  \n",
       "6499                                               nan              1  \n",
       "6500                                               nan              1  \n",
       "\n",
       "[6501 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Unigram LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = []\n",
    "all_tweets = []\n",
    "for i,tweet in text.iterrows():\n",
    "    if len(unigram) != 0:\n",
    "        all_tweets.append(unigram)\n",
    "    unigram= []\n",
    "    for word in tweet['processed'].split():\n",
    "        unigram.append(str(word))\n",
    "all_tweets.append(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(7520 unique tokens: ['base', 'becam', 'cell', 'cellag', 'cellbasedmeat']...) from 6248 documents (total 74569 corpus positions)\n",
      "INFO:gensim.models.ldamodel:using serial LDA version on this node\n",
      "INFO:gensim.models.ldamodel:running online (single-pass) LDA training, 14 topics, 1 passes over the supplied corpus of 6248 documents, updating model once every 2000 documents, evaluating perplexity every 6248 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #2000/6248\n",
      "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 6248 documents\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.010): 0.069*\"meat\" + 0.019*\"food\" + 0.014*\"futur\" + 0.013*\"make\" + 0.013*\"via\" + 0.013*\"product\" + 0.011*\"sustain\" + 0.011*\"cultur\" + 0.010*\"anim\" + 0.010*\"compani\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.010): 0.048*\"meat\" + 0.018*\"anim\" + 0.013*\"without\" + 0.013*\"make\" + 0.013*\"use\" + 0.012*\"way\" + 0.010*\"product\" + 0.009*\"innov\" + 0.009*\"food\" + 0.009*\"grow\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.010): 0.022*\"meat\" + 0.020*\"world\" + 0.017*\"anim\" + 0.012*\"food\" + 0.012*\"futur\" + 0.012*\"better\" + 0.012*\"make\" + 0.010*\"chicken\" + 0.009*\"cell\" + 0.009*\"work\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.010): 0.040*\"meat\" + 0.020*\"food\" + 0.017*\"futur\" + 0.015*\"sustain\" + 0.013*\"anim\" + 0.011*\"world\" + 0.009*\"work\" + 0.009*\"clean\" + 0.008*\"want\" + 0.008*\"get\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.010): 0.053*\"meat\" + 0.012*\"memphi\" + 0.012*\"anim\" + 0.011*\"food\" + 0.010*\"join\" + 0.010*\"futur\" + 0.008*\"new\" + 0.008*\"lab\" + 0.008*\"world\" + 0.008*\"campaign\"\n",
      "INFO:gensim.models.ldamodel:topic diff=90.925591, rho=1.000000\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #4000/6248\n",
      "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 6248 documents\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.010): 0.044*\"thank\" + 0.018*\"meat\" + 0.018*\"cto\" + 0.011*\"come\" + 0.010*\"scienc\" + 0.009*\"need\" + 0.009*\"industri\" + 0.009*\"food\" + 0.009*\"round\" + 0.008*\"friend\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.010): 0.023*\"meat\" + 0.022*\"wild\" + 0.018*\"sustain\" + 0.015*\"team\" + 0.012*\"excit\" + 0.011*\"cell\" + 0.010*\"base\" + 0.010*\"welcom\" + 0.009*\"bring\" + 0.009*\"world\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.010): 0.037*\"food\" + 0.031*\"meat\" + 0.015*\"futur\" + 0.015*\"seafood\" + 0.015*\"base\" + 0.014*\"industri\" + 0.012*\"cell\" + 0.012*\"fish\" + 0.009*\"great\" + 0.009*\"thank\"\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.010): 0.026*\"meat\" + 0.026*\"cell\" + 0.026*\"seafood\" + 0.018*\"base\" + 0.017*\"anim\" + 0.009*\"one\" + 0.009*\"clean\" + 0.009*\"team\" + 0.008*\"great\" + 0.008*\"year\"\n",
      "INFO:gensim.models.ldamodel:topic #13 (0.010): 0.029*\"meat\" + 0.023*\"cell\" + 0.019*\"base\" + 0.017*\"compani\" + 0.015*\"futureoffood\" + 0.012*\"thank\" + 0.010*\"food\" + 0.010*\"check\" + 0.009*\"talk\" + 0.009*\"stemcel\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.237586, rho=0.707107\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #6000/6248\n",
      "INFO:gensim.models.ldamodel:merging changes from 2000 documents into a model of 6248 documents\n",
      "INFO:gensim.models.ldamodel:topic #13 (0.010): 0.035*\"meat\" + 0.021*\"base\" + 0.019*\"compani\" + 0.017*\"plant\" + 0.017*\"futureoffood\" + 0.017*\"cell\" + 0.014*\"steak\" + 0.014*\"first\" + 0.012*\"culturedmeat\" + 0.012*\"year\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.010): 0.045*\"meat\" + 0.016*\"cultur\" + 0.015*\"new\" + 0.013*\"market\" + 0.013*\"year\" + 0.013*\"chicken\" + 0.012*\"base\" + 0.012*\"food\" + 0.011*\"cell\" + 0.008*\"lab\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.010): 0.031*\"thank\" + 0.020*\"meat\" + 0.015*\"coronaviru\" + 0.013*\"scienc\" + 0.012*\"cto\" + 0.011*\"round\" + 0.009*\"prepar\" + 0.009*\"award\" + 0.009*\"cultivatedmeat\" + 0.009*\"technion\"\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.010): 0.028*\"meat\" + 0.022*\"cell\" + 0.018*\"base\" + 0.017*\"seafood\" + 0.016*\"anim\" + 0.011*\"today\" + 0.009*\"year\" + 0.009*\"one\" + 0.008*\"world\" + 0.008*\"product\"\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.010): 0.061*\"steak\" + 0.024*\"cultivatedmeat\" + 0.023*\"foodtech\" + 0.023*\"world\" + 0.021*\"grown\" + 0.020*\"cleanmeat\" + 0.020*\"farm\" + 0.017*\"first\" + 0.016*\"pandem\" + 0.015*\"culturedmeat\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.182533, rho=0.577350\n",
      "INFO:gensim.models.ldamodel:-23.157 per-word bound, 9355482.4 perplexity estimate based on a held-out corpus of 248 documents with 2874 words\n",
      "INFO:gensim.models.ldamodel:PROGRESS: pass 0, at document #6248/6248\n",
      "INFO:gensim.models.ldamodel:merging changes from 248 documents into a model of 6248 documents\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.010): 0.037*\"meat\" + 0.024*\"product\" + 0.020*\"sale\" + 0.018*\"lab\" + 0.015*\"grown\" + 0.013*\"like\" + 0.013*\"launch\" + 0.012*\"interview\" + 0.012*\"wrap\" + 0.012*\"cell\"\n",
      "INFO:gensim.models.ldamodel:topic #13 (0.010): 0.030*\"meat\" + 0.030*\"futureoffood\" + 0.026*\"year\" + 0.022*\"cellag\" + 0.022*\"cleanmeat\" + 0.021*\"plant\" + 0.017*\"base\" + 0.016*\"cultur\" + 0.015*\"culturedmeat\" + 0.013*\"ago\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.010): 0.032*\"meat\" + 0.031*\"aleph\" + 0.023*\"food\" + 0.019*\"year\" + 0.016*\"age\" + 0.015*\"new\" + 0.015*\"farm\" + 0.013*\"job\" + 0.013*\"anim\" + 0.012*\"set\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.010): 0.042*\"food\" + 0.031*\"meat\" + 0.016*\"futur\" + 0.015*\"base\" + 0.014*\"industri\" + 0.011*\"technolog\" + 0.011*\"scienc\" + 0.011*\"excit\" + 0.010*\"look\" + 0.010*\"biolog\"\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.010): 0.029*\"meat\" + 0.022*\"cell\" + 0.016*\"anim\" + 0.016*\"base\" + 0.014*\"everi\" + 0.013*\"sign\" + 0.010*\"year\" + 0.010*\"seafood\" + 0.010*\"speci\" + 0.009*\"clean\"\n",
      "INFO:gensim.models.ldamodel:topic diff=0.230083, rho=0.500000\n",
      "INFO:gensim.models.ldamodel:topic #0 (0.010): 0.043*\"cleanmeat\" + 0.040*\"steak\" + 0.021*\"foodtech\" + 0.018*\"cellag\" + 0.017*\"futureoffood\" + 0.017*\"cultivatedmeat\" + 0.017*\"world\" + 0.016*\"culturedmeat\" + 0.016*\"first\" + 0.015*\"farm\" + 0.014*\"cellbasedmeat\" + 0.013*\"cm\" + 0.013*\"biotech\" + 0.012*\"food\" + 0.012*\"via\" + 0.012*\"meat\" + 0.011*\"cellularagricultur\" + 0.011*\"make\" + 0.011*\"cell\" + 0.011*\"grown\"\n",
      "INFO:gensim.models.ldamodel:topic #1 (0.010): 0.052*\"goodfoodconfer\" + 0.033*\"meat\" + 0.025*\"food\" + 0.023*\"plantbas\" + 0.021*\"billion\" + 0.014*\"get\" + 0.012*\"burger\" + 0.011*\"new\" + 0.011*\"sustain\" + 0.010*\"protein\" + 0.010*\"industri\" + 0.010*\"sept\" + 0.010*\"talk\" + 0.009*\"great\" + 0.009*\"impact\" + 0.008*\"anim\" + 0.008*\"see\" + 0.008*\"acceler\" + 0.008*\"futur\" + 0.008*\"action\"\n",
      "INFO:gensim.models.ldamodel:topic #2 (0.010): 0.037*\"meat\" + 0.024*\"product\" + 0.020*\"sale\" + 0.018*\"lab\" + 0.015*\"grown\" + 0.013*\"like\" + 0.013*\"launch\" + 0.012*\"interview\" + 0.012*\"wrap\" + 0.012*\"cell\" + 0.011*\"space\" + 0.011*\"plant\" + 0.010*\"replac\" + 0.010*\"star\" + 0.010*\"cleanmeat\" + 0.009*\"cultiv\" + 0.009*\"protein\" + 0.009*\"base\" + 0.009*\"learn\" + 0.009*\"super\"\n",
      "INFO:gensim.models.ldamodel:topic #3 (0.010): 0.063*\"meat\" + 0.026*\"food\" + 0.019*\"invest\" + 0.017*\"base\" + 0.016*\"new\" + 0.016*\"startup\" + 0.014*\"global\" + 0.014*\"product\" + 0.013*\"cell\" + 0.013*\"plant\" + 0.013*\"isra\" + 0.012*\"compani\" + 0.012*\"via\" + 0.012*\"rais\" + 0.012*\"clean\" + 0.012*\"fund\" + 0.011*\"sustain\" + 0.011*\"protein\" + 0.011*\"million\" + 0.011*\"world\"\n",
      "INFO:gensim.models.ldamodel:topic #4 (0.010): 0.042*\"food\" + 0.031*\"meat\" + 0.016*\"futur\" + 0.015*\"base\" + 0.014*\"industri\" + 0.011*\"technolog\" + 0.011*\"scienc\" + 0.011*\"excit\" + 0.010*\"look\" + 0.010*\"biolog\" + 0.009*\"investor\" + 0.009*\"plant\" + 0.009*\"innov\" + 0.009*\"cleanmeat\" + 0.009*\"tech\" + 0.009*\"clean\" + 0.009*\"program\" + 0.008*\"cultur\" + 0.008*\"seafood\" + 0.008*\"foodtech\"\n",
      "INFO:gensim.models.ldamodel:topic #5 (0.010): 0.032*\"meat\" + 0.031*\"aleph\" + 0.023*\"food\" + 0.019*\"year\" + 0.016*\"age\" + 0.015*\"new\" + 0.015*\"farm\" + 0.013*\"job\" + 0.013*\"anim\" + 0.012*\"set\" + 0.012*\"use\" + 0.011*\"happi\" + 0.010*\"autom\" + 0.010*\"per\" + 0.009*\"impact\" + 0.008*\"approach\" + 0.008*\"product\" + 0.008*\"take\" + 0.008*\"like\" + 0.008*\"global\"\n",
      "INFO:gensim.models.ldamodel:topic #6 (0.010): 0.027*\"sustain\" + 0.023*\"mission\" + 0.020*\"team\" + 0.020*\"excit\" + 0.018*\"via\" + 0.018*\"meat\" + 0.017*\"world\" + 0.017*\"say\" + 0.016*\"achiev\" + 0.016*\"wild\" + 0.016*\"synthet\" + 0.015*\"industri\" + 0.014*\"challeng\" + 0.013*\"scale\" + 0.011*\"manufactur\" + 0.011*\"differ\" + 0.011*\"parti\" + 0.010*\"consum\" + 0.010*\"vast\" + 0.010*\"caus\"\n",
      "INFO:gensim.models.ldamodel:topic #7 (0.010): 0.034*\"meat\" + 0.023*\"cleanmeat\" + 0.023*\"futurefood\" + 0.018*\"anim\" + 0.018*\"use\" + 0.016*\"cultivatedmeat\" + 0.016*\"food\" + 0.015*\"less\" + 0.014*\"plantbas\" + 0.013*\"cell\" + 0.012*\"protein\" + 0.012*\"muscl\" + 0.011*\"product\" + 0.011*\"grow\" + 0.011*\"opportun\" + 0.011*\"free\" + 0.010*\"public\" + 0.010*\"berkeley\" + 0.009*\"alt\" + 0.009*\"talk\"\n",
      "INFO:gensim.models.ldamodel:topic #8 (0.010): 0.025*\"thank\" + 0.019*\"congratul\" + 0.017*\"scienc\" + 0.016*\"award\" + 0.016*\"meat\" + 0.014*\"agtech\" + 0.014*\"focu\" + 0.013*\"standard\" + 0.010*\"includ\" + 0.010*\"come\" + 0.010*\"gfi\" + 0.009*\"open\" + 0.009*\"end\" + 0.009*\"million\" + 0.009*\"coronaviru\" + 0.009*\"professor\" + 0.009*\"industri\" + 0.009*\"univers\" + 0.008*\"progress\" + 0.008*\"fight\"\n",
      "INFO:gensim.models.ldamodel:topic #9 (0.010): 0.044*\"meat\" + 0.033*\"food\" + 0.018*\"futur\" + 0.017*\"cell\" + 0.015*\"protein\" + 0.014*\"join\" + 0.014*\"new\" + 0.014*\"base\" + 0.014*\"anim\" + 0.013*\"cultur\" + 0.011*\"dairi\" + 0.010*\"ceo\" + 0.010*\"industri\" + 0.010*\"cleanmeat\" + 0.009*\"farm\" + 0.009*\"sustain\" + 0.009*\"shrimp\" + 0.009*\"week\" + 0.009*\"milk\" + 0.008*\"plant\"\n",
      "INFO:gensim.models.ldamodel:topic #10 (0.010): 0.050*\"meat\" + 0.019*\"chicken\" + 0.017*\"cell\" + 0.017*\"base\" + 0.015*\"cultur\" + 0.014*\"year\" + 0.014*\"egg\" + 0.012*\"new\" + 0.012*\"market\" + 0.009*\"cleanmeat\" + 0.009*\"food\" + 0.009*\"produc\" + 0.009*\"consum\" + 0.009*\"engin\" + 0.009*\"demand\" + 0.008*\"world\" + 0.008*\"predict\" + 0.008*\"move\" + 0.008*\"restaur\" + 0.008*\"term\"\n",
      "INFO:gensim.models.ldamodel:topic #11 (0.010): 0.051*\"meat\" + 0.031*\"anim\" + 0.022*\"eat\" + 0.021*\"fish\" + 0.018*\"beef\" + 0.018*\"climat\" + 0.017*\"livestock\" + 0.016*\"cow\" + 0.015*\"goodfood\" + 0.015*\"chang\" + 0.015*\"tast\" + 0.013*\"percent\" + 0.013*\"antibiot\" + 0.013*\"environment\" + 0.012*\"world\" + 0.011*\"cleanmeat\" + 0.011*\"would\" + 0.011*\"human\" + 0.010*\"water\" + 0.010*\"tackl\"\n",
      "INFO:gensim.models.ldamodel:topic #12 (0.010): 0.029*\"meat\" + 0.022*\"cell\" + 0.016*\"anim\" + 0.016*\"base\" + 0.014*\"everi\" + 0.013*\"sign\" + 0.010*\"year\" + 0.010*\"seafood\" + 0.010*\"speci\" + 0.009*\"clean\" + 0.009*\"engin\" + 0.008*\"today\" + 0.008*\"earth\" + 0.008*\"effort\" + 0.008*\"huge\" + 0.008*\"grate\" + 0.008*\"cleanmeat\" + 0.007*\"agricultur\" + 0.007*\"product\" + 0.007*\"world\"\n",
      "INFO:gensim.models.ldamodel:topic #13 (0.010): 0.030*\"meat\" + 0.030*\"futureoffood\" + 0.026*\"year\" + 0.022*\"cellag\" + 0.022*\"cleanmeat\" + 0.021*\"plant\" + 0.017*\"base\" + 0.016*\"cultur\" + 0.015*\"culturedmeat\" + 0.013*\"ago\" + 0.013*\"compani\" + 0.011*\"beyond\" + 0.011*\"march\" + 0.011*\"seed\" + 0.010*\"talk\" + 0.010*\"think\" + 0.010*\"cell\" + 0.009*\"first\" + 0.009*\"soon\" + 0.009*\"agricultur\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.043*\"cleanmeat\" + 0.040*\"steak\" + 0.021*\"foodtech\" + 0.018*\"cellag\" + 0.017*\"futureoffood\" + 0.017*\"cultivatedmeat\" + 0.017*\"world\" + 0.016*\"culturedmeat\" + 0.016*\"first\" + 0.015*\"farm\" + 0.014*\"cellbasedmeat\" + 0.013*\"cm\" + 0.013*\"biotech\" + 0.012*\"food\" + 0.012*\"via\" + 0.012*\"meat\" + 0.011*\"cellularagricultur\" + 0.011*\"make\" + 0.011*\"cell\" + 0.011*\"grown\"'),\n",
       " (1,\n",
       "  '0.052*\"goodfoodconfer\" + 0.033*\"meat\" + 0.025*\"food\" + 0.023*\"plantbas\" + 0.021*\"billion\" + 0.014*\"get\" + 0.012*\"burger\" + 0.011*\"new\" + 0.011*\"sustain\" + 0.010*\"protein\" + 0.010*\"industri\" + 0.010*\"sept\" + 0.010*\"talk\" + 0.009*\"great\" + 0.009*\"impact\" + 0.008*\"anim\" + 0.008*\"see\" + 0.008*\"acceler\" + 0.008*\"futur\" + 0.008*\"action\"'),\n",
       " (2,\n",
       "  '0.037*\"meat\" + 0.024*\"product\" + 0.020*\"sale\" + 0.018*\"lab\" + 0.015*\"grown\" + 0.013*\"like\" + 0.013*\"launch\" + 0.012*\"interview\" + 0.012*\"wrap\" + 0.012*\"cell\" + 0.011*\"space\" + 0.011*\"plant\" + 0.010*\"replac\" + 0.010*\"star\" + 0.010*\"cleanmeat\" + 0.009*\"cultiv\" + 0.009*\"protein\" + 0.009*\"base\" + 0.009*\"learn\" + 0.009*\"super\"'),\n",
       " (3,\n",
       "  '0.063*\"meat\" + 0.026*\"food\" + 0.019*\"invest\" + 0.017*\"base\" + 0.016*\"new\" + 0.016*\"startup\" + 0.014*\"global\" + 0.014*\"product\" + 0.013*\"cell\" + 0.013*\"plant\" + 0.013*\"isra\" + 0.012*\"compani\" + 0.012*\"via\" + 0.012*\"rais\" + 0.012*\"clean\" + 0.012*\"fund\" + 0.011*\"sustain\" + 0.011*\"protein\" + 0.011*\"million\" + 0.011*\"world\"'),\n",
       " (4,\n",
       "  '0.042*\"food\" + 0.031*\"meat\" + 0.016*\"futur\" + 0.015*\"base\" + 0.014*\"industri\" + 0.011*\"technolog\" + 0.011*\"scienc\" + 0.011*\"excit\" + 0.010*\"look\" + 0.010*\"biolog\" + 0.009*\"investor\" + 0.009*\"plant\" + 0.009*\"innov\" + 0.009*\"cleanmeat\" + 0.009*\"tech\" + 0.009*\"clean\" + 0.009*\"program\" + 0.008*\"cultur\" + 0.008*\"seafood\" + 0.008*\"foodtech\"'),\n",
       " (5,\n",
       "  '0.032*\"meat\" + 0.031*\"aleph\" + 0.023*\"food\" + 0.019*\"year\" + 0.016*\"age\" + 0.015*\"new\" + 0.015*\"farm\" + 0.013*\"job\" + 0.013*\"anim\" + 0.012*\"set\" + 0.012*\"use\" + 0.011*\"happi\" + 0.010*\"autom\" + 0.010*\"per\" + 0.009*\"impact\" + 0.008*\"approach\" + 0.008*\"product\" + 0.008*\"take\" + 0.008*\"like\" + 0.008*\"global\"'),\n",
       " (6,\n",
       "  '0.027*\"sustain\" + 0.023*\"mission\" + 0.020*\"team\" + 0.020*\"excit\" + 0.018*\"via\" + 0.018*\"meat\" + 0.017*\"world\" + 0.017*\"say\" + 0.016*\"achiev\" + 0.016*\"wild\" + 0.016*\"synthet\" + 0.015*\"industri\" + 0.014*\"challeng\" + 0.013*\"scale\" + 0.011*\"manufactur\" + 0.011*\"differ\" + 0.011*\"parti\" + 0.010*\"consum\" + 0.010*\"vast\" + 0.010*\"caus\"'),\n",
       " (7,\n",
       "  '0.034*\"meat\" + 0.023*\"cleanmeat\" + 0.023*\"futurefood\" + 0.018*\"anim\" + 0.018*\"use\" + 0.016*\"cultivatedmeat\" + 0.016*\"food\" + 0.015*\"less\" + 0.014*\"plantbas\" + 0.013*\"cell\" + 0.012*\"protein\" + 0.012*\"muscl\" + 0.011*\"product\" + 0.011*\"grow\" + 0.011*\"opportun\" + 0.011*\"free\" + 0.010*\"public\" + 0.010*\"berkeley\" + 0.009*\"alt\" + 0.009*\"talk\"'),\n",
       " (8,\n",
       "  '0.025*\"thank\" + 0.019*\"congratul\" + 0.017*\"scienc\" + 0.016*\"award\" + 0.016*\"meat\" + 0.014*\"agtech\" + 0.014*\"focu\" + 0.013*\"standard\" + 0.010*\"includ\" + 0.010*\"come\" + 0.010*\"gfi\" + 0.009*\"open\" + 0.009*\"end\" + 0.009*\"million\" + 0.009*\"coronaviru\" + 0.009*\"professor\" + 0.009*\"industri\" + 0.009*\"univers\" + 0.008*\"progress\" + 0.008*\"fight\"'),\n",
       " (9,\n",
       "  '0.044*\"meat\" + 0.033*\"food\" + 0.018*\"futur\" + 0.017*\"cell\" + 0.015*\"protein\" + 0.014*\"join\" + 0.014*\"new\" + 0.014*\"base\" + 0.014*\"anim\" + 0.013*\"cultur\" + 0.011*\"dairi\" + 0.010*\"ceo\" + 0.010*\"industri\" + 0.010*\"cleanmeat\" + 0.009*\"farm\" + 0.009*\"sustain\" + 0.009*\"shrimp\" + 0.009*\"week\" + 0.009*\"milk\" + 0.008*\"plant\"'),\n",
       " (10,\n",
       "  '0.050*\"meat\" + 0.019*\"chicken\" + 0.017*\"cell\" + 0.017*\"base\" + 0.015*\"cultur\" + 0.014*\"year\" + 0.014*\"egg\" + 0.012*\"new\" + 0.012*\"market\" + 0.009*\"cleanmeat\" + 0.009*\"food\" + 0.009*\"produc\" + 0.009*\"consum\" + 0.009*\"engin\" + 0.009*\"demand\" + 0.008*\"world\" + 0.008*\"predict\" + 0.008*\"move\" + 0.008*\"restaur\" + 0.008*\"term\"'),\n",
       " (11,\n",
       "  '0.051*\"meat\" + 0.031*\"anim\" + 0.022*\"eat\" + 0.021*\"fish\" + 0.018*\"beef\" + 0.018*\"climat\" + 0.017*\"livestock\" + 0.016*\"cow\" + 0.015*\"goodfood\" + 0.015*\"chang\" + 0.015*\"tast\" + 0.013*\"percent\" + 0.013*\"antibiot\" + 0.013*\"environment\" + 0.012*\"world\" + 0.011*\"cleanmeat\" + 0.011*\"would\" + 0.011*\"human\" + 0.010*\"water\" + 0.010*\"tackl\"'),\n",
       " (12,\n",
       "  '0.029*\"meat\" + 0.022*\"cell\" + 0.016*\"anim\" + 0.016*\"base\" + 0.014*\"everi\" + 0.013*\"sign\" + 0.010*\"year\" + 0.010*\"seafood\" + 0.010*\"speci\" + 0.009*\"clean\" + 0.009*\"engin\" + 0.008*\"today\" + 0.008*\"earth\" + 0.008*\"effort\" + 0.008*\"huge\" + 0.008*\"grate\" + 0.008*\"cleanmeat\" + 0.007*\"agricultur\" + 0.007*\"product\" + 0.007*\"world\"'),\n",
       " (13,\n",
       "  '0.030*\"meat\" + 0.030*\"futureoffood\" + 0.026*\"year\" + 0.022*\"cellag\" + 0.022*\"cleanmeat\" + 0.021*\"plant\" + 0.017*\"base\" + 0.016*\"cultur\" + 0.015*\"culturedmeat\" + 0.013*\"ago\" + 0.013*\"compani\" + 0.011*\"beyond\" + 0.011*\"march\" + 0.011*\"seed\" + 0.010*\"talk\" + 0.010*\"think\" + 0.010*\"cell\" + 0.009*\"first\" + 0.009*\"soon\" + 0.009*\"agricultur\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=[]\n",
    "    for word in i['processed'].split():\n",
    "        unigram.append(word)\n",
    "    unigram_list.append(unigram)\n",
    "data_words = [i for i in unigram_list]\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=14,random_state=2022,alpha=0.01,eta=0.01,per_word_topics=True)\n",
    "lda_model.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 1000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 2000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 3000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 4000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 5000 documents\n",
      "INFO:gensim.topic_coherence.text_analysis:CorpusAccumulator accumulated stats from 6000 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-6.73612562449334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherencemodel = CoherenceModel(model=lda_model, texts=unigram_list, dictionary=id2word, coherence='u_mass')\n",
    "coherencemodel.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hellinger Distance:\n",
    "![HD.png](HD.png)\n",
    "# Entropy:\n",
    "![Entropy.png](Entropy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_entropy(topic_word_distribution,num_of_topics):\n",
    "    entropy = 0\n",
    "    for topic in topic_word_distribution:\n",
    "        for probability in topic:\n",
    "            entropy -= probability*math.log(probability)\n",
    "    return entropy/num_of_topics  \n",
    "from gensim.matutils import hellinger\n",
    "def calculate_hellinger_distance(topic_word,number_of_topics):\n",
    "    distance = 0\n",
    "    for i in range(0,number_of_topics-1):\n",
    "        for j in range(i+1,number_of_topics):\n",
    "            distance += hellinger(topic_word[i],topic_word[j])\n",
    "    return distance*2/(number_of_topics*(number_of_topics-1))\n",
    "def Evaluate(seedtopic,number_of_unseededtopic):\n",
    "    number_of_topic = len(seedtopic)+number_of_unseededtopic #seeded topics + 2 unseeded topics\n",
    "    model = guidedlda.GuidedLDA(n_topics=number_of_topic,n_iter=1000,random_state=2022,refresh=10,alpha=0.01,eta=0.01)#9 ok\n",
    "    seed_topics = {}\n",
    "    for t_id, st in enumerate(seedtopic):\n",
    "        for word in st:\n",
    "            seed_topics[word2id[word]] = t_id\n",
    "\n",
    "    data_words = [i for i in all_tweets]\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    from gensim.models import CoherenceModel\n",
    "    model = model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.7)\n",
    "    topic_word = model.topic_word_\n",
    "    coherencemodel_guided_cv_seed4 = CoherenceModel(model=model, texts=all_tweets, dictionary=id2word, coherence='c_v')\n",
    "    entropy_score = calculate_entropy(topic_word,number_of_topic)\n",
    "    hellinger_distance = calculate_hellinger_distance(topic_word,number_of_topic)\n",
    "    return coherencemodel_guided_cv_seed4.get_coherence() , entropy_score, hellinger_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(7520 unique tokens: ['base', 'becam', 'cell', 'cellag', 'cellbasedmeat']...) from 6248 documents (total 74569 corpus positions)\n",
      "INFO:guidedlda:n_documents: 6248\n",
      "INFO:guidedlda:vocab_size: 7520\n",
      "INFO:guidedlda:n_words: 74569\n",
      "INFO:guidedlda:n_topics: 14\n",
      "INFO:guidedlda:n_iter: 1000\n",
      "INFO:guidedlda:<0> log likelihood: -978900\n",
      "INFO:guidedlda:<10> log likelihood: -624493\n",
      "INFO:guidedlda:<20> log likelihood: -610005\n",
      "INFO:guidedlda:<30> log likelihood: -602178\n",
      "INFO:guidedlda:<40> log likelihood: -598280\n",
      "INFO:guidedlda:<50> log likelihood: -594905\n",
      "INFO:guidedlda:<60> log likelihood: -592685\n",
      "INFO:guidedlda:<70> log likelihood: -591103\n",
      "INFO:guidedlda:<80> log likelihood: -589407\n",
      "INFO:guidedlda:<90> log likelihood: -588374\n",
      "INFO:guidedlda:<100> log likelihood: -587394\n",
      "INFO:guidedlda:<110> log likelihood: -586658\n",
      "INFO:guidedlda:<120> log likelihood: -586134\n",
      "INFO:guidedlda:<130> log likelihood: -585331\n",
      "INFO:guidedlda:<140> log likelihood: -584887\n",
      "INFO:guidedlda:<150> log likelihood: -583957\n",
      "INFO:guidedlda:<160> log likelihood: -583905\n",
      "INFO:guidedlda:<170> log likelihood: -583959\n",
      "INFO:guidedlda:<180> log likelihood: -583448\n",
      "INFO:guidedlda:<190> log likelihood: -582732\n",
      "INFO:guidedlda:<200> log likelihood: -582772\n",
      "INFO:guidedlda:<210> log likelihood: -582866\n",
      "INFO:guidedlda:<220> log likelihood: -582306\n",
      "INFO:guidedlda:<230> log likelihood: -581861\n",
      "INFO:guidedlda:<240> log likelihood: -582152\n",
      "INFO:guidedlda:<250> log likelihood: -581384\n",
      "INFO:guidedlda:<260> log likelihood: -581281\n",
      "INFO:guidedlda:<270> log likelihood: -581545\n",
      "INFO:guidedlda:<280> log likelihood: -581178\n",
      "INFO:guidedlda:<290> log likelihood: -581182\n",
      "INFO:guidedlda:<300> log likelihood: -580344\n",
      "INFO:guidedlda:<310> log likelihood: -580531\n",
      "INFO:guidedlda:<320> log likelihood: -580248\n",
      "INFO:guidedlda:<330> log likelihood: -580262\n",
      "INFO:guidedlda:<340> log likelihood: -580152\n",
      "INFO:guidedlda:<350> log likelihood: -580038\n",
      "INFO:guidedlda:<360> log likelihood: -580054\n",
      "INFO:guidedlda:<370> log likelihood: -579703\n",
      "INFO:guidedlda:<380> log likelihood: -579626\n",
      "INFO:guidedlda:<390> log likelihood: -579829\n",
      "INFO:guidedlda:<400> log likelihood: -579267\n",
      "INFO:guidedlda:<410> log likelihood: -579421\n",
      "INFO:guidedlda:<420> log likelihood: -579592\n",
      "INFO:guidedlda:<430> log likelihood: -579405\n",
      "INFO:guidedlda:<440> log likelihood: -579929\n",
      "INFO:guidedlda:<450> log likelihood: -579438\n",
      "INFO:guidedlda:<460> log likelihood: -579081\n",
      "INFO:guidedlda:<470> log likelihood: -579332\n",
      "INFO:guidedlda:<480> log likelihood: -579168\n",
      "INFO:guidedlda:<490> log likelihood: -579181\n",
      "INFO:guidedlda:<500> log likelihood: -579398\n",
      "INFO:guidedlda:<510> log likelihood: -579310\n",
      "INFO:guidedlda:<520> log likelihood: -579122\n",
      "INFO:guidedlda:<530> log likelihood: -579173\n",
      "INFO:guidedlda:<540> log likelihood: -579279\n",
      "INFO:guidedlda:<550> log likelihood: -579127\n",
      "INFO:guidedlda:<560> log likelihood: -579411\n",
      "INFO:guidedlda:<570> log likelihood: -578957\n",
      "INFO:guidedlda:<580> log likelihood: -578758\n",
      "INFO:guidedlda:<590> log likelihood: -578822\n",
      "INFO:guidedlda:<600> log likelihood: -579205\n",
      "INFO:guidedlda:<610> log likelihood: -578270\n",
      "INFO:guidedlda:<620> log likelihood: -578556\n",
      "INFO:guidedlda:<630> log likelihood: -578266\n",
      "INFO:guidedlda:<640> log likelihood: -578478\n",
      "INFO:guidedlda:<650> log likelihood: -578571\n",
      "INFO:guidedlda:<660> log likelihood: -578318\n",
      "INFO:guidedlda:<670> log likelihood: -578337\n",
      "INFO:guidedlda:<680> log likelihood: -578395\n",
      "INFO:guidedlda:<690> log likelihood: -578306\n",
      "INFO:guidedlda:<700> log likelihood: -578306\n",
      "INFO:guidedlda:<710> log likelihood: -578469\n",
      "INFO:guidedlda:<720> log likelihood: -578630\n",
      "INFO:guidedlda:<730> log likelihood: -578364\n",
      "INFO:guidedlda:<740> log likelihood: -578323\n",
      "INFO:guidedlda:<750> log likelihood: -578373\n",
      "INFO:guidedlda:<760> log likelihood: -578039\n",
      "INFO:guidedlda:<770> log likelihood: -577938\n",
      "INFO:guidedlda:<780> log likelihood: -578049\n",
      "INFO:guidedlda:<790> log likelihood: -578555\n",
      "INFO:guidedlda:<800> log likelihood: -578525\n",
      "INFO:guidedlda:<810> log likelihood: -578390\n",
      "INFO:guidedlda:<820> log likelihood: -578376\n",
      "INFO:guidedlda:<830> log likelihood: -578214\n",
      "INFO:guidedlda:<840> log likelihood: -578549\n",
      "INFO:guidedlda:<850> log likelihood: -578131\n",
      "INFO:guidedlda:<860> log likelihood: -578151\n",
      "INFO:guidedlda:<870> log likelihood: -577951\n",
      "INFO:guidedlda:<880> log likelihood: -578166\n",
      "INFO:guidedlda:<890> log likelihood: -577957\n",
      "INFO:guidedlda:<900> log likelihood: -578107\n",
      "INFO:guidedlda:<910> log likelihood: -577805\n",
      "INFO:guidedlda:<920> log likelihood: -577982\n",
      "INFO:guidedlda:<930> log likelihood: -578176\n",
      "INFO:guidedlda:<940> log likelihood: -577830\n",
      "INFO:guidedlda:<950> log likelihood: -578002\n",
      "INFO:guidedlda:<960> log likelihood: -577709\n",
      "INFO:guidedlda:<970> log likelihood: -577843\n",
      "INFO:guidedlda:<980> log likelihood: -577914\n",
      "INFO:guidedlda:<990> log likelihood: -577740\n",
      "INFO:guidedlda:<999> log likelihood: -577826\n",
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:7 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 1774 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7881332790526823, 6.012459103383283, 0.8141072818656245)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate(bigramseed,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = lda_model.get_topics() #The topic-word distribution return by traditional LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.569550429546264"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(lda_topics,14) #Calculate the entropy of the results of traditional LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7084897920519022"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_hellinger_distance(lda_topics,14) #Calculate the hellinger distance of the results of traditional LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(7520 unique tokens: ['base', 'becam', 'cell', 'cellag', 'cellbasedmeat']...) from 6248 documents (total 74569 corpus positions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\\n    coherence_values = []\\n    model_list = []\\n    for num_topics in range(start,limit,step):\\n        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,random_state=2022)\\n        model_list.append(model)\\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=\\'c_v\\')\\n        coherence_values.append(coherencemodel.get_coherence())\\n    return model_list, coherence_values\\n#coherence score\\nlimit=21; start=10; step=1\\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\\n# Show graph\\nimport matplotlib.pyplot as plt\\nx = range(start, limit, step)\\nplt.plot(x, coherence_values,label=\\'All tweets\\')\\nplt.xlabel(\"Num Topics\")\\nplt.ylabel(\"Coherence score\")\\nplt.legend(loc=\\'best\\')\\nplt.show()'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unigram\n",
    "from gensim.models import CoherenceModel\n",
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=[]\n",
    "    for word in i['processed'].split():\n",
    "        unigram.append(word)\n",
    "    unigram_list.append(unigram)\n",
    "data_words = [i for i in unigram_list]\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "texts = data_words\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "'''def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start,limit,step):\n",
    "        model=gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,random_state=2022)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "#coherence score\n",
    "limit=21; start=10; step=1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values,label='All tweets')\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.topic_coherence.probability_estimation:using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:serializing accumulator to return to master...\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulator serialized\n",
      "INFO:gensim.topic_coherence.text_analysis:7 accumulators retrieved from output queue\n",
      "INFO:gensim.topic_coherence.text_analysis:accumulated word occurrence stats for 5740 virtual documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3497456953504089"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CoherenceModel(model=lda_model, texts=all_tweets, dictionary=id2word, coherence='c_v').get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To export tweets within each topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_topic0 = text2.loc[text2['most_salient_topic']== 0].sort_values('probability',ascending=False)\n",
    "text_topic1 = text2.loc[text2['most_salient_topic']== 1].sort_values('probability',ascending=False)\n",
    "text_topic2 = text2.loc[text2['most_salient_topic']== 2].sort_values('probability',ascending=False)\n",
    "text_topic3 = text2.loc[text2['most_salient_topic']== 3].sort_values('probability',ascending=False)\n",
    "text_topic4 = text2.loc[text2['most_salient_topic']== 4].sort_values('probability',ascending=False)\n",
    "text_topic5 = text2.loc[text2['most_salient_topic']== 5].sort_values('probability',ascending=False)\n",
    "text_topic6 = text2.loc[text2['most_salient_topic']== 6].sort_values('probability',ascending=False)\n",
    "text_topic7 = text2.loc[text2['most_salient_topic']== 7].sort_values('probability',ascending=False)\n",
    "text_topic8 = text2.loc[text2['most_salient_topic']== 8].sort_values('probability',ascending=False)\n",
    "text_topic9 = text2.loc[text2['most_salient_topic']== 9].sort_values('probability',ascending=False)\n",
    "text_topic10 = text2.loc[text2['most_salient_topic']== 10].sort_values('probability',ascending=False)\n",
    "text_topic11 = text2.loc[text2['most_salient_topic']== 11].sort_values('probability',ascending=False)\n",
    "text_topic12 = text2.loc[text2['most_salient_topic']== 12].sort_values('probability',ascending=False)\n",
    "text_topic13 = text2.loc[text2['most_salient_topic']== 13].sort_values('probability',ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
