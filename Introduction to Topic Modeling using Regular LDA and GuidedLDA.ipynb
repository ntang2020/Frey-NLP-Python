{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING\n",
    "\n",
    "Topic Modelling is the task of using unsupervised learning to extract the main topics (represented as a set of words) that occur in a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working Problem\n",
    "\n",
    "Today, I will be showing how to use regular LDA and guidedLDA for topic modeling and I tested the LDA algorithm on 20 Newsgroup data present in sklearn set which has thousands of news articles from many sections of a news report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation ##\n",
    "\n",
    "LDA is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. \n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Therefore choosing the right corpus of data is crucial. \n",
    "* It also assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset\n",
    "\n",
    "The dataset we'll use is the 20newsgroup dataset that is available from sklearn. This dataset has news articles grouped into 20 news categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314,) (11314,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\"] [7]\n"
     ]
    }
   ],
   "source": [
    "# Lets look at some sample news and target groups\n",
    "print(newsgroups_train.data[:1],newsgroups_train.target[:1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names)), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### As you can see that there are some distinct themes in the news categories like \n",
    "\n",
    "* sports \n",
    "* religion\n",
    "* science \n",
    "* technology\n",
    "* politics etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simple Preprocessing ##\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "* **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All **stopwords** are removed.\n",
    "* Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are **stemmed** - words are reduced to their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lubasisikwibele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the Stemmer\n",
    "Let's also look at a stemming example. Let's throw a number of words at the stemmer and see how it deals with each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "def stemming_and_lemmatizing(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocessing(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(stemming_and_lemmatizing(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 50\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocessing(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now preprocess all the news headlines we have. To do that, we iterate over the list of documents in our training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for doc in newsgroups_train.data:\n",
    "    docs.append(preprocessing(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview 'docs' to see what all words are being captured\n",
    "'''\n",
    "print(docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "capturing all the different words present in 'docs'\n",
    "'''\n",
    "all_word_corpus = []\n",
    "for doc in docs:\n",
    "    all_word_corpus += doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Length of docs vs. Various Percentiles **\n",
    "\n",
    "Filter out docs that have\n",
    "\n",
    "* less than a certain size (absolute number) or\n",
    "* more than a certain size (fraction of total corpus size, not absolute number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = pd.Series([len(sent) for sent in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11314.000000\n",
       "mean       126.780272\n",
       "std        234.656018\n",
       "min          9.000000\n",
       "0%           9.000000\n",
       "10%         37.000000\n",
       "20%         48.000000\n",
       "30%         58.000000\n",
       "40%         69.000000\n",
       "50%         81.000000\n",
       "60%         94.000000\n",
       "70%        114.000000\n",
       "80%        144.000000\n",
       "90%        213.000000\n",
       "100%      5494.000000\n",
       "max       5494.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lengths.describe(percentiles=[x/10 for x in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11314.000000\n",
       "mean       126.780272\n",
       "std        234.656018\n",
       "min          9.000000\n",
       "0%           9.000000\n",
       "1%          20.000000\n",
       "2%          23.000000\n",
       "3%          27.000000\n",
       "4%          28.000000\n",
       "5%          30.000000\n",
       "6%          32.000000\n",
       "7%          33.000000\n",
       "8%          34.000000\n",
       "9%          36.000000\n",
       "50%         81.000000\n",
       "max       5494.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing 0% to 10% percentiles\n",
    "doc_lengths.describe(percentiles=[x/100 for x in range(0,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11314.000000\n",
       "mean       126.780272\n",
       "std        234.656018\n",
       "min          9.000000\n",
       "50%         81.000000\n",
       "90%        213.000000\n",
       "91%        224.000000\n",
       "92%        241.000000\n",
       "93%        262.090000\n",
       "94%        288.000000\n",
       "95%        319.000000\n",
       "96%        367.000000\n",
       "97%        426.610000\n",
       "98%        558.220000\n",
       "99%        885.000000\n",
       "max       5494.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing 90% to 100% percentiles\n",
    "doc_lengths.describe(percentiles=[x/100 for x in range(90,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosing upper_bound = 300 and lower_bound = 30\n",
    "chosen_docs = []\n",
    "for doc in docs:\n",
    "    if len(doc) > 30 and len(doc) < 300:\n",
    "        chosen_docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 10063\n"
     ]
    }
   ],
   "source": [
    "print(len(docs), len(chosen_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = chosen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "capturing all the different words present in 'docs'\n",
    "'''\n",
    "all_word_corpus = []\n",
    "for doc in docs:\n",
    "    all_word_corpus += doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating Corpus Before LDA ##\n",
    "\n",
    "Some of the initialisations necessary for LDA are:\n",
    "* **id2word** is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **word2id** is a mapping from words (strings) to word ids (integers). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "* **vocab** is a list of words (strings). It is used to determine the various words used in the text, as well as for debugging and topic printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "vocab = []\n",
    "currentWordId = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for word, count in Counter(all_word_corpus).most_common():\n",
    "    word2id[word] = currentWordId\n",
    "    id2word[currentWordId] = word\n",
    "    currentWordId += 1\n",
    "    vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 61411, 61411)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), len(word2id), len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    currentDoc = []\n",
    "    for word in doc: \n",
    "        if word in word2id:\n",
    "            currentDoc.append(word2id[word])\n",
    "    final_docs.append(currentDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = final_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Running LDA using Bag of Words ##\n",
    "\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "Only parameters we will be tweaking are:\n",
    "\n",
    "* **num_topics** is the number of requested latent topics to be extracted from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(docs), len(word2id)),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 61411)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(docs):\n",
    "    for idy in doc:\n",
    "        X[idx,idy] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 : Regular LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install guidedlda\n",
    "from lda import guidedlda as glda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(reversed(model.word_topic_.T[1].argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda import guidedlda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidedlda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_27700/3378359881.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mguidedlda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGuidedLDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'guidedlda'"
     ]
    }
   ],
   "source": [
    "from guidedlda import GuidedLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 : Guided LDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_dict = {'Graphics Cards' : 0, 'Space' : 1, 'Religion' : 2 , 'Politics' : 3, 'Gun Violence' : 4,\n",
    "             'Technology' : 5, 'Sports' : 6, 'Encryption' : 7 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 0: Possibly Graphics Cards\n",
    "seed_topics.update({\n",
    "    word2id[\"drive\"] : seed_dict[\"Graphics Cards\"], word2id[\"sale\"] : seed_dict[\"Graphics Cards\"], \n",
    "    word2id[\"driver\"] : seed_dict[\"Graphics Cards\"], word2id[\"wire\"] : seed_dict[\"Graphics Cards\"], \n",
    "    word2id[\"card\"] : seed_dict[\"Graphics Cards\"], word2id[\"graphic\"] : seed_dict[\"Graphics Cards\"], \n",
    "    word2id[\"price\"] : seed_dict[\"Graphics Cards\"], word2id[\"appl\"] : seed_dict[\"Graphics Cards\"],\n",
    "    word2id[\"softwar\"] : seed_dict[\"Graphics Cards\"], word2id[\"monitor\"] : seed_dict[\"Graphics Cards\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 2: Possibly Space\n",
    "seed_topics.update({\n",
    "    word2id[\"space\"] : seed_dict[\"Space\"], word2id[\"nasa\"] : seed_dict[\"Space\"], \n",
    "    word2id[\"drive\"] : seed_dict[\"Space\"], word2id[\"scsi\"] : seed_dict[\"Space\"], \n",
    "    word2id[\"orbit\"] : seed_dict[\"Space\"], word2id[\"launch\"] : seed_dict[\"Space\"],\n",
    "    word2id[\"data\"] : seed_dict[\"Space\"], word2id[\"control\"] : seed_dict[\"Space\"], \n",
    "    word2id[\"earth\"] : seed_dict[\"Space\"],word2id[\"moon\"] : seed_dict[\"Space\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 6: Possibly Sports\n",
    "seed_topics.update({\n",
    "    word2id[\"game\"] : seed_dict[\"Sports\"], word2id[\"team\"] : seed_dict[\"Sports\"], \n",
    "    word2id[\"play\"] : seed_dict[\"Sports\"], word2id[\"player\"] : seed_dict[\"Sports\"], \n",
    "    word2id[\"hockey\"] : seed_dict[\"Sports\"], word2id[\"season\"] : seed_dict[\"Sports\"], \n",
    "    word2id[\"pitt\"] : seed_dict[\"Sports\"], word2id[\"score\"] : seed_dict[\"Sports\"], \n",
    "    word2id[\"leagu\"] : seed_dict[\"Sports\"], word2id[\"pittsburgh\"] : seed_dict[\"Sports\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 4: Possibly Politics\n",
    "seed_topics.update({\n",
    "    word2id[\"armenian\"] : seed_dict[\"Politics\"], word2id[\"public\"] : seed_dict[\"Politics\"], \n",
    "    word2id[\"govern\"] : seed_dict[\"Politics\"], word2id[\"turkish\"] : seed_dict[\"Politics\"], \n",
    "    word2id[\"columbia\"] : seed_dict[\"Politics\"], word2id[\"nation\"] : seed_dict[\"Politics\"], \n",
    "    word2id[\"presid\"] : seed_dict[\"Politics\"], word2id[\"turk\"] : seed_dict[\"Politics\"], \n",
    "    word2id[\"american\"] : seed_dict[\"Politics\"], word2id[\"group\"] : seed_dict[\"Politics\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic 5: Possibly Gun Violence\n",
    "seed_topics.update({\n",
    "    word2id[\"kill\"] : seed_dict[\"Gun Violence\"], word2id[\"bike\"] : seed_dict[\"Gun Violence\"], \n",
    "    word2id[\"live\"] : seed_dict[\"Gun Violence\"], word2id[\"leav\"] : seed_dict[\"Gun Violence\"], \n",
    "    word2id[\"weapon\"] : seed_dict[\"Gun Violence\"], word2id[\"happen\"] : seed_dict[\"Gun Violence\"], \n",
    "    word2id[\"gun\"] : seed_dict[\"Gun Violence\"], word2id[\"crime\"] : seed_dict[\"Gun Violence\"],\n",
    "    word2id[\"car\"] : seed_dict[\"Gun Violence\"], word2id[\"hand\"] : seed_dict[\"Gun Violence\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 11314\n",
      "INFO:lda:vocab_size: 61411\n",
      "INFO:lda:n_words: 1434392\n",
      "INFO:lda:n_topics: 8\n",
      "INFO:lda:n_iter: 1000\n",
      "INFO:lda:<0> log likelihood: -16188091\n",
      "INFO:lda:<50> log likelihood: -12449256\n",
      "INFO:lda:<100> log likelihood: -12346263\n",
      "INFO:lda:<150> log likelihood: -12304848\n",
      "INFO:lda:<200> log likelihood: -12284159\n",
      "INFO:lda:<250> log likelihood: -12268061\n",
      "INFO:lda:<300> log likelihood: -12259401\n",
      "INFO:lda:<350> log likelihood: -12255148\n",
      "INFO:lda:<400> log likelihood: -12251819\n",
      "INFO:lda:<450> log likelihood: -12247948\n",
      "INFO:lda:<500> log likelihood: -12245794\n",
      "INFO:lda:<550> log likelihood: -12246700\n",
      "INFO:lda:<600> log likelihood: -12244817\n",
      "INFO:lda:<650> log likelihood: -12238984\n",
      "INFO:lda:<700> log likelihood: -12235704\n",
      "INFO:lda:<750> log likelihood: -12234234\n",
      "INFO:lda:<800> log likelihood: -12233578\n",
      "INFO:lda:<850> log likelihood: -12232271\n",
      "INFO:lda:<900> log likelihood: -12230158\n",
      "INFO:lda:<950> log likelihood: -12228875\n",
      "INFO:lda:<999> log likelihood: -12229782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic 0 : window, file, line, subject, organ, program, write, post, problem, work, version, host, like, know, card, imag, drive, need, graphic, mail, nntp, univers, scsi, avail, help\n",
      "\n",
      "\n",
      "Topic 1 : space, nasa, organ, line, subject, research, orbit, year, post, univers, center, launch, write, program, data, articl, develop, inform, scienc, access, work, includ, earth, satellit, time\n",
      "\n",
      "\n",
      "Topic 2 : informatik, hamburg, appear, intercon, bontchev, nrhj, amanda, dresden, wwiz, organ, fbihh, repli, navi, vote, subject, gizw, vesselin, cover, germani, comic, copi, bhjn, wolverin, walker, line\n",
      "\n",
      "\n",
      "Topic 3 : peopl, govern, right, write, state, armenian, organ, subject, line, articl, encrypt, say, know, think, israel, secur, public, like, time, post, isra, presid, chip, american, clipper\n",
      "\n",
      "\n",
      "Topic 4 : write, line, articl, subject, organ, like, peopl, think, know, go, post, time, say, thing, good, host, nntp, come, right, year, want, look, tell, bike, univers\n",
      "\n",
      "\n",
      "Topic 5 : write, peopl, christian, think, subject, line, organ, know, believ, articl, say, jesus, like, post, mean, exist, univers, thing, time, come, question, moral, reason, bibl, word\n",
      "\n",
      "\n",
      "Topic 6 : game, team, line, organ, subject, play, year, player, write, articl, univers, post, think, hockey, season, like, host, nntp, good, score, time, leagu, basebal, know, go\n",
      "\n",
      "\n",
      "Topic 7 : line, subject, organ, post, univers, host, nntp, write, articl, drive, know, like, distribut, need, sale, thank, work, good, price, mail, look, want, power, repli, engin\n"
     ]
    }
   ],
   "source": [
    "model = glda.GuidedLDA(n_topics= 8, n_iter=1000, random_state=1, refresh=50)\n",
    "model.fit(X, seed_topics = seed_topics, seed_confidence = 0.3)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 25\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words + 1):-1]\n",
    "    print('\\n')\n",
    "    print('Topic {} : {}'.format(i, ', '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing model on unseen document ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: help\n",
      "From: C..Doelle@p26.f3333.n106.z1.fidonet.org (C. Doelle)\n",
      "Lines: 13\n",
      "\n",
      "Hello All!\n",
      "\n",
      "    It is my understanding that all True-Type fonts in Windows are loaded in\n",
      "prior to starting Windows - this makes getting into Windows quite slow if you\n",
      "have hundreds of them as I do.  First off, am I correct in this thinking -\n",
      "secondly, if that is the case - can you get Windows to ignore them on boot and\n",
      "maybe make something like a PIF file to load them only when you enter the\n",
      "applications that need fonts?  Any ideas?\n",
      "\n",
      "\n",
      "Chris\n",
      "\n",
      " * Origin: chris.doelle.@f3333.n106.z1.fidonet.org (1:106/3333.26)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num = 100\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = preprocessing(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow() missing 1 required positional argument: 'document'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_27700/959404895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow() missing 1 required positional argument: 'document'"
     ]
    }
   ],
   "source": [
    "bow_vector = gensim.corpora.Dictionary.doc2bow(document,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pc/vkjk_mj1775gl8p0wvydr_600000gn/T/ipykernel_27700/741495734.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score: {}\\t Topic: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_test.target[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with 'x'% probability to the X category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
